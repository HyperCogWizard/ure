
                                   Triples
                                   -------
                         Linas Vepstas January 2009

This directory contains some experimental code to extract conceptual 
entities and semantic triples from English text.  An example of a 
"conceptual entity" would be the "Great Southern Railroad": a business,
a railway, that existed at a certain point in space and time. A semantic
triple is a triple of subject-predicate-object, and is popularly the 
topic of "Semantic Web RDF" technology discussions[WP-RDF].  An example 
of a "semantic triple" is "the sky is blue". Written in prefix notation,
the triple is then "color_of(sky, blue)", with "color_of" being the
property or predicate, while "sky" and "blue" are the subject and
object, respectively.

The primary challenges here are three-fold:
1) Dealing with the diversity of expression in the English language, 
   where most sentences are NOT short, to-the-point assertions that
   something is true. For short, simple sentences, ReLex seems to
   provide enough normalization to extract at least some triples. But 
   it is not clear how far this idea can be pushed.
2) Learning new relations. It is simple to code up, by hand, "framing"
   relationships, such as a list of colors, or a list properties that
   an object can have. It is harder to determine, when encountering a 
   new word, to determine that it is, for example, a color.
3) Constructing a data representation that is ameanable to reasoning,
   and to question answering. 

ToDo: Reification of triples ... 
Phrasal verbs vs. prepositional phrases 

Structures
----------
Triples are to follow the existing opencog predicate structure:

   (EvaluationLink
      (ConceptNode "color_of")
      (ListLink
         (ConceptNode "sky")
         (ConceptNode "blue")
      )
   )

is an example of what would be deduced from the copula "The color of the
sky is blue". This example is somewhat over-simplified; this is
addressed later.

Simples Example Sentences
-------------------------
First, consider some input sentences:

Input sentence: "The capital of Germany is Berlin."
Relex output: 
_subj(be, capital)
_obj(be, Berlin)
of(capital, Germany)

Input sentence: "Berlin is the capital of Germany."
_subj(be, Berlin)
_obj(be, capital)
of(capital, Germany)

Input sentence: "The color of the sky is blue."
of(color, sky)
_predadj(blue, color)

Input sentence: "Pottery is made from clay."
_obj(make, pottery)
from(make, clay)

Input sentence: "Yarn is spun from fibers."
_obj(spin, yarn)
from(spin, fibers)

Input sentence: "Yarn is made of fibers."
_obj(made_of, yarn)
_iobj(made_of, fibers)

Input sentence: "Berlin is in Germany."
The more highly ranked parse gives:
in(_%copula, in)
_pobj(in, Germany)
_psubj(in, Berlin)

A second, but lower-ranked parse, gives:
in(be, Germany)
_subj(be, Berlin)

Input sentence: "Berlin is a city in Germany."
in(be, Germany)
_subj(be, Berlin)
_obj(be, city)

A second, but lower-ranked parse, gives:
in(city, Germany)
_subj(be, Berlin)
_obj(be, city)

Clearly, even simple assertions have many different forms.

A priori vs. Deduced Knowledge
------------------------------
Consider again the following:

_subj(be, capital)
_obj(be, Berlin)
of(capital, Germany)

This sentence references a lot of a-priori knowledge.  We know that
capitals are cities; thus there is a strong temptation to write a
processing rule such as "IF ($var0,capital) THEN ($var0,city)".
Similarly, one has a-priori knowledge that things which have capitals
are political states, and so one is tempted to write a rule asserting
this: "IF (capital_of($var0, $var1)) THEN political_state($var1)".

A current working assumption of what follows is that the normalization
rules will attempt to encode a minimum of a-priori "real-world" knowledge.
Instead, the goal here is to encode linguistic knowledge, and
specifically, linguistic knowledge pertaining to prepositions and
copulas: the copula expresses "is-a" relations, while the preposition
expresses relationships: "has-a", "next-to", "part-of", "made-of", etc.

The hope is that, by encoding the relatively small number of
prepositional relationships, the much larger set of "real-world"
knowledge rules can be deduced (via backward or forward chaining).

Learning Semantic Categories
----------------------------
Consider the category of "types of motion". Currently, the RelEx frame
rules include an explicit list of category members:  

$Self_motion
amble
bustle
canter
clamber
climb
clomp
coast
crawl
creep

This list clearly encodes a-priori knowledge about locomotion.  It would
be better if the members of this category could be deduced by reading.
There are three ways in which this might be done. One might someday
read a sentence that asserts "Crawling is a type of locomotion".  This
seems unlikely, as this is common-sense knowledge, and common-sense
knowledge is not normally encoded in text. A second possibility is to
learn the meaning of the word "crawl" the way that children learn it: 
to have someone point at a centipede and say "gee, look at that thing
crawl!"  Such experiential, cross-sensory learning would indeed be an
excellent way to gain new knowledge. However, there are two snags: 
1) It presumes the existence of a teacher who already knows how to use
the word "crawl", and 2) It is outside of the scope of what one person
(i.e. me) can acheive in a limited amount of time.  A third possibility
is statistical learning: to observe a large number os statements
containing the word "crawl", and, based on these, deduce that it is a
type of locomotion.

In the following, the third approach is presumed. This is because the
author has in hand both the statistical and the linguistic tools that
would allow such observation and deduction to be made.

Rules
-----
Consider again the following:

   Input sentence: "The capital of Germany is Berlin."
   _subj(be, capital)
   _obj(be, Berlin)
   of(capital, Germany)

and we wish to deduce:

   captial_of(Germany, Berlin)

The following processing rule acheives this:

   # IF _subj(be,$var0) ^ _obj(be,$var1) ^ $prep($var0,$var2) THEN 
     ^3_$var0_$prep($var2, $var1)

The carats ^ in the pedicate denote 'and'. Dollar signs preceed variable
names. In the predicand, the initial ^3_ denotes the origin of the rule.
(^1_ denoes framenet, ^2_ denoes David Noziglia, and ^3_ deones Linas).
The above IF-THEN rule is intended to be identical in syntax to the
rules that are already used in the RelEx framing code; i.e. the syntax
is meant to be identical to that of the file "data/frame/mapping_rules.txt"
in the RelEx source tree.

The above rule cannot be applied to the following:

   Input sentence: "Berlin is the capital of Germany."
   _subj(be, Berlin)
   _obj(be, capital)
   of(capital, Germany)

because of the constraint that $var0 appear in both the preposition
and the subject.  Proper mangling of the above requires the following
rule:

   # IF _subj(be,$var0) ^ _obj(be,$var1) ^ $prep($var1,$var2) THEN 
     ^3_$var1_$prep($var2, $var0)

Notice the connection across var0 and var1 is symmetrically exchanged.


Encoding as an ImplicationLink
------------------------------
The above rules are to be encoded as ImplicationLinks. Thus, the rule

   # IF _subj(be,$var0) ^ _obj(be,$var1) ^ $prep($var0,$var2) THEN 
     ^3_$var0_$prep($var2, $var1)

is meant to be a short-hand for:

   ImplicationLink
      AndLink
         EvaluationLink
            DefinedLinguisticRelationshipNode "_subj"
            ListLink
               ConceptNode "be"
               VariableNode "$var0"
         EvaluationLink
            DefinedLinguisticRelationshipNode "_obj"
            ListLink
               ConceptNode "be"
               VariableNode "$var1"
         EvaluationLink
            VariableNode "$prep"
            ListLink
               VariableNode "$var0"
               VariableNode "$var2"
      EvaluationLink
         DefinedLinguisticRelationshipNode ($var0 . "_" . $prep)
         ListLink
            VariableNode "$var2"
            VariableNode "$var1"

Note the snag of concatenating strings.  The only easy way out of
this appears to be to salt the atomspace with contractions.  Thus,
the atomspace would contain a relation:

   EvaluationLink
      DefinedLinguisticRelationshipNode "capital_of"
      ListLink
         ConceptNode "capital"
         DefinedLinguisticRelationshipNode "of"

The above ImplicationLink would then include a fourth clause in the
predicate:

   EvaluationLink
      VariableNode $phrase
      ListLink
         VariableNode $var0
         VariableNode $prep

while the implicand would be written as:

   EvaluationLink
      VariableNode $phrase
      ListLink
         VariableNode "$var2"
         VariableNode "$var1"

The above would be expressed directly in the rules syntax as:

   # IF _subj(be,$var0) ^ _obj(be,$var1) ^ $prep($var0,$var2) 
       ^ $phrase($var0, $prep)
       THEN ^3_$phrase($var2, $var1)

There is one additional complication to the above. The natural output of
Relex puts a buffer between words and concepts. Thus, and example of the 
output is:

(EvaluationLink
   (DefinedLinguisticRelationshipNode "of")
   (ListLink
      (WordInstanceNode "capital@cd67b274-9957-463c-aad8-422bec133613")
      (WordInstanceNode "Germany@75f2f934-91a7-47ba-9bcc-e2a3340c9076")
   )
)

The word instances are related to thier lemmatized forms:

(LemmaLink
   (WordInstanceNode "capital@cd67b274-9957-463c-aad8-422bec133613")
   (WordNode "capital")
)
(LemmaLink
   (WordInstanceNode "Germany@75f2f934-91a7-47ba-9bcc-e2a3340c9076")
   (WordNode "Germany")
)

Thus, the implication predicate needs to bind word-instances to word
nodes. 

There is another problem: ImplicationLinks cannot be used to change 
the type of a node. Thus, while the examples above made reference to
ConceptNodes, the actual RelEx LemmaLinks specify WordNodes.  There's 
no immediate, direct way to turn WordNodes into ConceptNodes; this 
will be fudged for now.

And there is a another problem, a tricky one: it only makes sense to 
extract relations between words in a sentence (or across sentences, 
if reference resolution is implemented).  However, it seems to be
somwhat tricky to specify that the searches should be performed only
on a certain subset of the atom space -- so, at this point, the
atomspace can only contain one, single sentence for processing.  
Otherwise, if it contains multiple sentences, the rules can
accidentally find matches to relations from different sentences.

Running the current code
------------------------
The file "rules.txt" contains the current set of rules.  The perl script
"rules-to-implications.pl" will convert rules into OpenCog
ImplicationLinks, as described above. 

Implications can be processed with the scheme interpreter, using the 
ad-hoc command:

  (cog-ad-hoc "do-implication" stmt)

where the stmt is the implication to evaluate.


Promoting Words to Concepts
---------------------------
There is one more aspect to this, which is much deeper: the promotion
of word-relations to concepts.  Consider the following relationships:

   is_a(bark, sound)
   part_of(bark, tree)

We know that these two relations refer to different senses of the word
"bark". Yet, if these two are deduced by reading, how should the system
recognize that two different concepts are at play?  How should the
self-consistency of a set of relations be assessed? Assuming that the
input text is not intentionally lying, then, under what circumstances
do a set of confflicting assertions require that the underlying word be
recognized as embodying two different concepts?

Several approaches are possible:
1) Assign tentative word-senses via the Mihalcea algorithm.
   Unfortunately, this algorithm is quite slow.  This can be partly
   worked around using the syntax-tagged senses taken from a database.
   This is extremely fast, however, its not terribly accurate; nor is
   the Mihalcea algorithm all that accurate either.  None-the-less
   its a reasonable starting point, perhaps.

2) Verify consistency via first-order-logic and forward/backward
   chaining. This has several dificulties. One is the problem of
   performance, and bounding the search space. Another is that, if
   an inconsistency is detected, its still not clear what faulty
   assumption lead to it.  There is no algorithm for classifying 
   a set of conflicting statements into two groups of self-consistent
   statements.

   (Possibly need to re-examin Markov Logic Networks?)


Deduction
---------
Suppose we have a set of (consistent) prepositional relationships. What
can we do with them?  For example, can we deduce that a certain verb is
a type of locomotion, based on its use with regard to prepositions?

Hmm. Time to write some rules, and experiment and see what happens.
Not clear how unambiguous the copulas and preps will be.


References:
-----------
[WP-RDF] Resource Description Framework
     http://en.wikipedia.org/wiki/Resource_Description_Framework

