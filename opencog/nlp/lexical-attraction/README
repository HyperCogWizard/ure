
                      Counting Statistics
                      -------------------
                   Linas Vepstas February 2009

Until now, statistical data gathering in the OpenCog NLP pipeline has
required the creation of SQL databases to the collected information.
These databases have been in purely custom-built to hold information
on the specific items of interest; they are not general in any way, and
require considerable effort for each new relation. The goal of the work
here is to provide a uniform infrastructure for statistical data
gathering and processing within OpenCog.

Statistics gathered for NLP input can be used to discern patterns and
relationships in the input.  An example of such are minimum-spanning-tree
dependency parsers, such as the Yuret algorithm, described below. More
generally, the concept of hierarchical mutual information can be used
to identify data clusters, thus extracting semantic content in a
statistical manner.

Statistics gathering are to be done using the CountTruthValue class,
which includes a counter, together with atom persistence, allowing for
a much larger hypergraph than what can be kept in RAM.  As text is
processed, the relevent atoms are pulled in from persistent store,
thier counts updated, and then stored back.  It is eventually planned
to tie this to the short-term importance mechanism for RAM management.

At this time, all data mining, including the computation of conditional
probabilities, mutual information, and hierarchical clustering, is
envisioned to be done off-line.

Hypergraphs specific to NLP
---------------------------

Below follows some of the relations of current interest. Statistical
information will be kept by maintaining counts on all of the links and
nodes.  This should be sufficeint for computing conditional
probabilities, mutual information and entropy.

Consider first word-pairs:

	WordPairLink  ; CountTruthValue stores count
		WordNode "red"
		WordNode "baloon"

To determine the proability P(left,right), one needs only the count
on the WordPairLink, since P(left,right) = N(left,right) / N(*,*)
That is, the counts on the individual WordNodes are not needed to
obtain the word-pair probabilities, mutual information, etc.

The reason use a WordPairLink, as opposed to recycling some existing
link, is to avoid garbaging the data by having some other counting
process accidentally make use of this link, and improperly increment
its count.

One minimal way of refining the "sense" or "meaning" of a word is to
identify it's part-of-speech. Thus, one is interested in word-pairs
tagged with part-of-speech. To generalize this, one considers word
lemmas (word roots or word stems), tagged with part-of-speech, and
maybe tagged with tense, number, etc.

	FeatureLink                      ; should this be PartOfSpeechLink?
		WordNode "refer"
		DefinedLinguisticConceptNode "verb"   ; part of speech tag

	FeatureLink
		WordNode "refer"
		DefinedLinguisticConceptNode "infinitive"  ; verb tense tag

	FeatureLink
		WordNode "refer"
		DefinedLinguisticConceptNode ".v"  ; the link-grammar "inflection"


The above should be gathered together into a SetLink, as the relative
order is not important.

Thus, pairs of words might be gathered together as:

	CountLink  ; holds CountTruthValue for counting
		SetLink
			FeatureLink
				WordNode "refer"
				DefinedLinguisticConceptNode "verb"   ; part of speech tag
			FeatureLink
				WordNode "refer"
				DefinedLinguisticConceptNode "infinitive"  ; verb tense tag
			FeatureLink
				WordNode "refer"
				DefinedLinguisticConceptNode ".v"  ; the link-grammar "inflection"
		SetLink
			FeatureLink
				WordNode "to"
				DefinedLinguisticConceptNode "prep"   ; part of speech tag


Of some interest is the connection between grammatical usage/construction,
and word sense.  That is, the occurrence of triples of the form
(inflected-word, word-sense, disjunct). These are used to narrow down
the likelyhood of a given word sense, given that it was used with a
certain disjunct.  That is, we are interested in the conditional
probability P(sense |word, disjunct)

CountLink                                ; kind-of-like an evaluation link
    DisjunctNode "S- O+ "                ; link-grammar disjunct
    ListLink
       WordNode "blah.n"                 ; inflected word
       WordSenseNode "blah::1:32:0:0.0"  ; wordnet sense

Also of interest is not only how syntax affixes meaning to a single
word, but also how syntax is used to narrow down meanings of pairs
of words. In particular, we expect to see a lot of collcations here,
and in particular, collocations involving noun-modifiers. Link-grammar
uses "AN+" links to denote noun-modifier phrases, e.g. "hand grip",
"lead glass", "soup bowl", where the modifier is a noun.

Thus, we are lead to consider pairs of the above:

CountLink
    LinkGrammarRelationshipNode "MX"
    CountLink
        DisjunctNode "S- O+ MX+ "
        ListLink
           WordNode "blah.n"
           WordSense "blah::1:32:0:0.0"
    CountLink
        DisjunctNode "S- MX- O+ "
        ListLink
           WordNode "glub.n"
           WordSense "glub::1:47:0:0.0"


Gathering Statistics
--------------------
The statistics are meant to be gathered by running the "standard" NLP
processing pipeline, which currently culminates with word-sense
identification. After word-sense assignments, CountLinks as defined
above are to be generated.  The easiest way to assemble these
particular hypergraphs from the mish-mash of input hypergraphs is to
use the query/forward-chainer to perform the assembly.  The result
of assembly will probably be a hypergraph that is sequestered in the
persistence (SQL) database. This hypergraph will be automatically
retreived by the atomspace, using the "BackingStore" mechanism. After
identification and retreival, the count on the count link needs to be
updated, and then saved back to the backing store.  At this point, its
acceptable to delete the count link structures, so as to save on RAM
usage.




=======================================================================




                        Lexical Attraction
                        ------------------
                     Linas Vepstas June 2008

This directory contains a prototype implementation of Yuret's "lexical
attraction" algorithm for the discovery of synatctic links between
words in a sentence [Yuret 1998]. The goal of this algorithm is to
provide a score for ranking the output of the link-grammar parser.

So far, nothing has been implemented; this is a pre-prototype.


Data Structures
---------------
The Yuret algorithm requires, as input, the mutual information between
pairs of words. Computing the mutal information requires performing a
statistical analysis on a large corpus of sentences. The statistical
analysis needs to maintain four counts:
-- n(x,y) = the number of times a word pair was observed
-- n(x,*) = the number of times a word occured as the left element of a
            word pair
-- n(*,y) = the number of times a word occured as the right element of
            a word pair
-- n(*,*) = N = the number of word pairs observed.

The joint probability of seeing a word pair (x,y) is then

  p(x,y) = n(x,y) / n(*,*)

The marginal probabilities of seeing a word on the left, or right,
respectively, are

  p(x,*) = n(x,*) / n(*,*)
  p(*,y) = n(*,y) / n(*,*)

The "lexical attraction" is defined by Yuret as [chapter 4, page 40]:

   LA(x,y) = log_2 p(x,y) / (p(x,*) p(*,y))

Yuret calls this the "mutual information" for the word pair; however,
this usage, while in a certain sense is correct, is also in conflict
with the common definition of mutual information as the expectation
value of LA; that is, as:

  MI(x,y) = E[LA(x,y)] = p(x,y) LA(x,y)

See for example, wikipedia "mutual information" or [Ash, Chapter 1.5]




Misc comments
-------------

A classic parse-ranking example is given in the following sentence.
Both parses are more or less legitimate, and have valid interpretations.

(S (NP I) (VP saw (NP the man) (PP with (NP the telescope))) .)

(S (NP I) (VP saw (NP (NP the man) (PP with (NP the telescope)))) .)



References
----------
See also probablistic context-free grammars.
E. Charniak, 1997. Statistical parsing with a context-free grammar
and word statistics. AAAI 1997.

* Robert B. Ash, ''Information Theory'' (1965) Dover publications.
* Deniz Yuret, ''Discovery of Linguistic Relations Using Lexical
  Attraction'' (1998) PhD Thesis
  http://citeseer.ist.psu.edu/yuret98discovery.html

