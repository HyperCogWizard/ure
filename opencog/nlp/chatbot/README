
                       La Cogita Chatbot
                       -----------------
                Linas Vepstas <linasvepstas@gmail.com>
                          April 2009

A quick-n-dirty hookup of the opencog internals to an IRC-based chatbot.

Currently, the bot is simply a protocol translator from the IRC protocol,
to opencog i/o. It is subject to change.

Configuring: 
-----------
There is an assortment of hard-coded configuration stuff in the source
code.  See go-irc.c for setable values for the name of the chatbot, the
irc channel and irc network to join, and the bot ID strings, etc.

By default, it connects to the #opencog channel on freenode.net.

The bot tries to connect to an opencog server at port 17004. This port 
number is hard-coded in whirr-sockets.cc.

Running:
--------
After modifying the hard-coded config as desired, start the bot by 
saying "opencog/nlp/chatbot/cogita". It should then appear on the IRC
channel. You can then talk to it by addressing it by name, i.e. 
prefacing comments with cog:, cogita: or cogita-bot:

To have it do something, you must have an opencog server running on
port 17004, and have the opencog server appropriately loaded up with
assorted databses, code, etc.

You also have to have the latest version of RelEx installed, and then
run ./opencog-server.sh from the main RelEx directory. This will create
a parse server on port 4444: it will read plain-text input, and generate
parsed text in opencog format.  The chatbot needs this server to process
text.

This is most easiloy done by starting the cogserver using the 
lib/opencog4.conf configuration file, which sets the correct port,
silences the opencog prompt, and loads the needed scheme files.

In addition, triples-processing, described in step 4 below, requires
that the shell script opencog/nlp/triples/load4.sh be run.

To enable the common-sense database (described in step 5), you must
create and load up the common-sense database, as follows:

cog-server> sql-open triples linas asdf
cog-server> sql-load


Architecture:
-------------
The current architecture makes use of several servers interconnected
by TCP/IP sockets to perform NLP processing.  The infrastructure is
currently very minimalistic, and is just barely enough to get the job
done. All modifications require tampering the source code. The pipeline
is as follows:

1) IRC I/O, performed by cogita, which acts as a intermediary between
   IRC and OpenCog.  It listens for input on an IRC channel, and 
   forwards the resulting plain-text to opencog. This is done by
   issuing one simple scheme expression to opencog, via the opencog
   command-line interface/scheme shell on port 17004. The expression 
   is ''(say-id-english usernick text)'', where the ''usernick'' is 
   the user's IRC nick, and ''text'' is what the user entered.  The
   return value from this command is sent back to the IRC channel.
   Communications is stateless and blocking: cogita closes the socket
   to indeicate end-of-messsage, and expects that the cog server will
   to the same. Only after the cog-server closes its socket does cogita
   reply on the IRC channel.

   IRC.cc,.h:  C++ cl;ass for generic IRC communications.
   go-irc.cc:  the main guts of the cogita server
   whirr-sockets.cc,.h: tcp socket to send data to opencog, get reply.  

   Note that if the cog-server is busy, then whirr can block for an
   indefinitely long time. The person who is chatting will start to 
   wonder about the lack of response. This should be fixed -- best
	fix is probably to multi-thread, while periodically telling the
	user that the cog server is thinking ...  XXX FIXME

2) Parsing of english text by RelEx. The very first thing that the 
   ''say-id-english'' routine does is to send the input text to RelEx
   for parsing.  This is done by sending the plain-text via a socket
   to a listening RelEx server; the server responds with the parsed
   text, in the form of a hypergraph of atoms expressed in scheme. 
   The hypergraph is then loaded into the opencog atomspace, and is
   ready for processing. 

   I/O to parser is stateless: RelEx will close the socket after it has
   completed its parse and returned its results. 

2a) Data flow.  Each newly parsed sentence is attached, via a ListLink, 
   to an AnchorNode "# New Parsed Sentence".  Later processing stages 
   use the same mechanism, of linking to AnchorNode's, to pass data to 
   each other.

3) Question-Answering. The bot can recognize some simple questions and
   answer them. This is doen by a very simple form of pattern-matching
   on RelEx output. Thus, for example, the dependency parse of "John
   threw the ball" results in "_subj(throw,John) _obj(throw,ball)".
   The question "Who threw a ball?" results in "_subj(throw,who) 
   _obj(throw,ball)". By taking "who" to mean "an unknown variable", 
   it is straight-forward to pattern-match the statement to the question,
   and determine that "who" corresponds to "John".

   The set-up for RelEx-based matching is done in nlp/question/RelexQuery.cc.
   The actual pattern-matching is done by generic hypergraph predicate
   maching code in query/PatternMatchEngine.cc.

4) Semantic triples. The scope of questions that can be answered can be
   slightly broadened by converting them into a normalized form, here
   refered to as a "semantic triple." An example would be the 
   prepositional phrase "color_of(sky,blue)", a form that is shared by
   both the statement and question "The color of the sky is blue" and
   "What is the color of the sky?" even though the RelEx dependency 
   parses of the statement and the question are quite different.
   
   Code to extract and create such triples is in the nlp/triples 
   directory. The pattern-matching for such triples is done by 
   nlp/question/TripleQuery.cc.  This code is triggered and run
   if the process in step-3 fails for find an answer (although
   triples are extracted for all new statements, in case they are
   needed for question-answering later.

5) Common sense database. The semantic-triple format allows a 
   repository of common-sense triples to be created, and questions 
   to be answered from such previously-read input. A dataset of
   common-sense assertions, written in English, is available from the
   MIT ConceptNet project. It contains sentences such as 
   "Baseball is a sport."  This dataset may be loaded into OpenCog by 
   following the instructions in nlp/triples/README.  Alternately, an
   SQL dump, suitable for immediate use, can be downloaded from 

http://gnucash/linas/nlp/xxxxxxxxxxxx
       http://XXXXXX (to-do)

   If the database has been loaded into OpenCog, then the same code as
   described in the previous step, above, will look for answers to 
   questions there.
   
6) Frames.  (Not hooked up) Additional context can be provided by 
   frame-net-like frames. There's code in RelEx for this, its not
   hooked up.
    
7) Reasoning (not implemented). In principle, reasoning could be done.
   Right now, there is no infrastructure for this, aside from the core
   reasoning engine, PLN itself.  All details for how to go about doing
   this are yet to be fleshed out.

8) NLgen (not yet hooked up). Natural language output.
   Code at https://launchpad.net/nlgen 
   Overview at http://www.opencog.org/wiki/SegSim



