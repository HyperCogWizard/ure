
                       La Cogita Chatbot
                       -----------------
                Linas Vepstas <linasvepstas@gmail.com>
                          August 2009

XXX As of this instant, important parts of the chatbot are being
re-designed, and so things sort of don't work. I beleive that
revision 3150 has a working chatbot.

A quick-n-dirty hookup of the OpenCog NLP pipeline to an IRC chatbot.

At this time, the bot is a demo of the OpenCog natural-language 
processing (NLP) pipeline; it is NOT -- repeat NOT -- a demo of
OpenCog reasoning, deduction, or anything like that.  The chatbot 
is as dumb as a rock, as thick as a brick; it possesses no intelligence
whatsoever.

It can, however, answer simple English-language questions. It does so
by comparing the syntactical structure of the question to previously
entered sentences.  Thus, for example, if the chatbot is told that 
"John threw a rock", and then asked "Who threw a rock?", direct 
comparison allows it to equate "Who" -> "John".  The comparisons are
done on the syntactic structure (dependency parse) of the sentence:
that is, the parsed form of the sentences and questions are compared.
This allows for some sophistication: the system can correctly answer
"What did John throw?" for the above input, because dependency parsing
will correctly identify "throw" as the head verb of the sentence.

Since OpenCog can save its contents to a database, it can "remember" 
a large number of assertions that have been previously entered, and 
answer questions about those.  Currently, a pre-parsed copy of the 
MIT ConceptNet can be loaded into OpenCog.

Please note: this is meant to be a DEMO of the NLP pipeline, and litle
more. It is up to you to do something with it!  Also: it is subject to 
change at any time, as I am likely to twiddle, tweak and change it about.
(Adding reasoning is a difficult but important priority).

Running the chatbot requires three steps:
1) Running the parser server.
2) Running the opencog server.
3) Running the chat-to-opencog bridge.

Run the parser server:
----------------------
The chatbot requires the English parser daemon to be running.  The 
parser is available in the newer versions of RelEx, and is started by
running ./opencog-server.sh from the main RelEx directory. (You must
compile RelEx first, by installing java and ant, and saying "ant" to
build RelEx).

The parse server listens on port 4444: it will read plain-text input, 
and generates parsed text in opencog format.  The chatbot needs this
server to process text.

There is a simple "sniff-test" that can be used to verify that the 
server is running: try "telnet localhost 4444" to connect to the server,
then type in an English sentence, then hit return. It should respond
with a verbose parse of the sentence.

Run the OpenCog server:
-----------------------
The chatbot requires an opencog server running on port 17004. This 
server needs to be appropriately configured and loaded with assorted 
databases, code, etc.

This is most easily done by starting the cogserver using the 
"lib/opencog-chatbot.conf" configuration file, which sets the correct
port, silences the opencog prompt, and loads the needed scheme files.

To enable the common-sense database (described in step 5), you must
first create and populate the database. This can be done from scratch,
or by using a database dump, obtained from 

       http://gnucash.org/linas/nlp/data/conceptnet/

The database should be created and populated, as described in 
opencog/nlp/triples/README.  The config file "lib/opencog-chatbot.conf"
needs to be edited, and the database name and login credentials updated.


Configure and run the chat bridge: 
----------------------------------
The chatbot itself is just a simple deamon that bridges text from the
chat system to the OpenCog server. The cogita chatbot is tailored for
IRC chat; other chat systems could be supported in a straightforward
manner. 

The IRC chatbot has an assortment of hard-coded config parameters in the
source code.  See CogitaConfig.cc for setable values for the name of the 
chatbot, the irc channel and irc network to join, and the bot ID strings,
etc.  By default, it connects to the #opencog channel on freenode.net.

The bot tries to connect to an opencog server at port 17004. This port 
number is hard-coded in whirr-sockets.cc.

After modifying the hard-coded config as desired, start the bot by 
saying "opencog/nlp/chatbot/cogita". It should then appear on the IRC
channel. You can then talk to it by addressing it by name, i.e. 
prefacing comments with cog:, cogita: or cogita-bot:


Architecture:
-------------
The current architecture makes use of several servers interconnected
by TCP/IP sockets to perform NLP processing.  The infrastructure is
currently very minimalistic, and is just barely enough to get the job
done. All modifications require tampering the source code. The pipeline
is as follows:

1) IRC I/O, performed by cogita, which acts as a intermediary between
   IRC and OpenCog.  It listens for input on an IRC channel, and 
   forwards the resulting plain-text to opencog. This is done by
   issuing one simple scheme expression to opencog, via the opencog
   command-line interface/scheme shell on port 17004. The expression 
   is ''(say-id-english usernick text)'', where the ''usernick'' is 
   the user's IRC nick, and ''text'' is what the user entered.  The
   return value from this command is sent back to the IRC channel.
   Communications is stateless and blocking: cogita closes the socket
   to indicate end-of-messsage, and expects that the cog server will
   do the same. Only after the cog-server closes its socket does cogita
   reply on the IRC channel.

   IRC.cc,.h:  C++ class for generic IRC communications.
   go-irc.cc:  the main guts of the cogita server
   whirr-sockets.cc,.h: tcp socket to send data to opencog, get reply.  

   Note that if the cog-server is busy, then whirr can block for an
   indefinitely long time. The person who is chatting will start to 
   wonder about the lack of response. This is currently hacked around
   by having the chat processing periodically return to the bridge,
   and then resume again. A proper architecture remains unimplemented.

2) Parsing of english text by RelEx. The very first thing that the 
   ''say-id-english'' routine does is to send the input text to RelEx
   for parsing.  This is done by sending the plain-text via a socket
   to a listening RelEx server; the server responds with the parsed
   text, in the form of a hypergraph of atoms expressed in scheme. 
   The hypergraph is then loaded into the opencog atomspace, and is
   ready for processing. 

   I/O to parser is stateless: RelEx will close the socket after it has
   completed its parse and returned its results. 

2a) Data flow.  Each newly parsed sentence is attached, via a ListLink, 
   to an AnchorNode "# New Parsed Sentence".  Later processing stages 
   use the same mechanism, of linking to AnchorNode's, to pass data to 
   each other.

3) Question-Answering. The bot can recognize some simple questions and
   answer them. This is doen by a very simple form of pattern-matching
   on RelEx output. Thus, for example, the dependency parse of "John
   threw the ball" results in "_subj(throw,John) _obj(throw,ball)".
   The question "Who threw a ball?" results in "_subj(throw,who) 
   _obj(throw,ball)". By taking "who" to mean "an unknown variable", 
   it is straight-forward to pattern-match the statement to the question,
   and determine that "who" corresponds to "John".

   The set-up for RelEx-based matching is done in nlp/question/RelexQuery.cc.
   The actual pattern-matching is done by generic hypergraph predicate
   maching code in query/PatternMatchEngine.cc.

4) Semantic triples. The scope of questions that can be answered can be
   slightly broadened by converting them into a normalized form, here
   refered to as a "semantic triple." An example would be the 
   prepositional phrase "color_of(sky,blue)", a form that is shared by
   both the statement and question "The color of the sky is blue" and
   "What is the color of the sky?" even though the RelEx dependency 
   parses of the statement and the question are quite different.
   
   Code to extract and create such triples is in the nlp/triples 
   directory. The pattern-matching for such triples is done by 
   nlp/question/TripleQuery.cc.  This code is triggered and run
   if the process in step-3 fails for find an answer (although
   triples are extracted for all new statements, in case they are
   needed for question-answering later.

5) Common sense database. The semantic-triple format allows a 
   repository of common-sense triples to be created, and questions 
   to be answered from such previously-read input. A dataset of
   common-sense assertions, written in English, is available from the
   MIT ConceptNet project. It contains sentences such as 
   "Baseball is a sport."  This dataset may be loaded into OpenCog by 
   following the instructions in nlp/triples/README.  Alternately, 
   SQL dumps, suitable for immediate use, can be downloaded from 

       http://gnucash.org/linas/nlp/data/conceptnet/

   If the database has been opened, then the same code as described in 
   the previous step, above, will look for answers to questions there.
   
6) Frames.  (Not hooked up) Additional context can be provided by 
   frame-net-like frames. There's code in RelEx for this, its not
   hooked up.
    
7) Reasoning (not implemented). In principle, reasoning could be done.
   Right now, there is no infrastructure for this, aside from the core
   reasoning engine, PLN itself.  All details for how to go about doing
   this are yet to be fleshed out.

8) NLgen/NLGen2 (not yet hooked up). Natural language output.
   Code at https://launchpad.net/nlgen 
   Overview at http://www.opencog.org/wiki/SegSim


Note that steps 3,4,5,6,7 very very crudely echo the kind of design
discussed in [Katz05].

Examples:
---------
The following dialog works (or has worked at various times in the past):

<me>         John threw a green ball.
<me>         Fred threw a red ball
<me>         Mary threw a blue rock
<me>         who threw a ball?
<cogita-bot> Fred John
<me>         who threw a red ball?
<cogita-bot> Fred



References
==========
[Katz05] Boris Katz, Gary Borchardt and Sue Felshin
    "Syntactic and Semantic Decomposition Strategies for Question
     Answering from Multiple Resources"
     Proceedings of the AAAI 2005 Workshop on Inference 2005 - aaai.org


http://en.wikipedia.org/wiki/MultiNet

Initial draft: April 2009 Linas Vepstas
Updated: August 2009 Linas Vepstas
