                          Question Answering
                          ------------------
               Linas Vepstas <linasvepstas@gmail.com>
                        Created 18 March 2008
                        Revised on 13 May 2008
                        Revised on 7 May 2009

This directory contains code that implements natural language question
answering via subgraph isomorphism. This README file describes the 
operation and reports on experimental results. The experiments start with
a fairly simple question-answering technique, which, although functional,
is not very robust. The basic idea is that NL parses of assertions, such
as "John threw the ball", and questions, such as "Who threw the ball?"
resemble each other quite closely. One need only recognize "who" as an
implicit variable name, and then search over the entire atomspace to 
find a matching pattern, at which point the word "John" can be used 
as a grounding for the variable "who?". Additonal experiments examine
increasingly more complex strategies.

Four sets of experiments are reported: pattern matching at the RelEx
level, pattern matching at the framenet level, pattern matching at 
the "semantic triples" level, and using predicate structures for 
sophisticated question processing. The RelEx level represents sentences
as collections of subject, object relations, together with noun number, 
verb tense, etc. markup.  The Frame level decorates the basic RelEx 
relations with situational context infered from the sentences. The
"semantic triples" normalize syntactic relations into "semantic-web"
style relational triples.  Finally, "question comprehension" recognizes
the structure of questions themselves, and respond to that structure.

XXX As of 13 May 2008, changes to the RelEx output have broken the 
operation of the frame-query part of this code. As of 23 April 2009,
th simpler RelEx query style has been resotred and now works again. XXX

General conclusion: while superficial question answering is possible, 
a robust system remains difficult.  Work continues (very slowly) along
several directions, and/or should explore several ideas: (1) identify
synonymous phrases, as described by Lin & Patel [Lin2001]. (2) Continue
work on "semantic normalization", along the lines of the "semantic 
triples" work described in nlp/triples/README. (3) Pursue use of PLN
for answer-generation (4) Pursue work on "question comprehension".

* [Lin2001] Dekang Lin and Patrick Pantel. 2001.  "Discovery of Inference
  Rules for Question Answering." ''Natural Language Engineering'' 
  '''7'''(4):343-360.  http://www.cs.ualberta.ca/~lindek/papers/jnle01.pdf 

Question Answering Overview
---------------------------
Curreently, the question answering code dependings on a processing
pipeline

 -- Input of English language sentences
 -- parsing by Link Grammar parser
 -- dependency relations built by RelEx
 -- framenet relations build by RelEx
 -- import into OpenCog
 -- semantic triple extraction within opencog
 -- "question comprehension" within opencog
 -- NL generation, English output.

The "chatbot" (nlp/chatbot/README) provides the overall systems 
integration that allows this pipeline to function.

Four approaches to question-answering are considered below. These all
make use of pattern-matching in one way or another; they differ in the
kinds of patterns that are searched for, and the sophistication of the 
pattens themselves.  These are:

 -- Pattern matching of RelEx dependency grammar expressions
 -- Pattern matching of FrameNet semantic frames
 -- Pattern matching on "semantic triples"
 -- "Question comprehension" by extracting question patterns

Query processing at the RelEx level provides only a very narrow, literal
answer; there is very little "wiggle room" or ability to understand the
nature of the question.  Thus, for example, the question "What did John
throw?" is represented by the RelEx dependency-grammar as

    _subj(throw, John )
    _obj(throw, _$qVar)

To answer this question, the OpenCog atomspace must contain a sentence
that had parsed into exactly the above form. For this example, the 
atomspace must contain the dependency parse for the sentence "John
threw a ball"

    _subj(throw, John )
    _obj(throw, ball)

From this, one can immediately induce that _$qVar == ball and thus the
answer is "ball".


Thus, a looser matching, based on deeper semantic understanding,
is desirable.  This is afforded by using the semantic framing 
matcher.


RelEx pattern matching
----------------------
A detailed example of RelEx relationship matching follows.

"John threw a ball."

_subj(throw, John)
_obj(throw, ball)
tense(throw, past)
noun_number(ball, singular)
DEFINITE-FLAG(John, T)
noun_number(John, singular)

"What did John throw?"

_subj(throw, John)
_obj(throw, _$qVar)
tense(throw, past_infinitive)
HYP(throw, T)
QUERY-TYPE(_$qVar, what)
DEFINITE-FLAG(John, T)
noun_number(John, singular)

Eliminate the HYP and QUERY-TYPE from the question, since
the answer will never contain these. One is left with:

_subj(throw, John)
_obj(throw, _$qVar)
tense(throw, past_infinitive)
DEFINITE-FLAG(John, T)
noun_number(John, singular)

Clearly, the RelEx pattern of the question can be matched to 
the RelEx pattern of the answer, to find that _$qVar == ball.
(Some twiddling required to match tense correctly).
This type of RelEx relation-level setup, query massaging 
and node matching is implemented in SentenceQuery.cc, SentenceQuery.h
It seems to work, but hasn't undergone strenuous testing.

Pattern matching at the RelEx level is not always possible, as shown
next. At the root of the problem is that RelEx provides only a small
amount of "semantic normalization". That is, RelEx will abstract 
(pull out, normalize) some, but not much, semantics.  Similar 
statements will be similar, whereas the query processor is looking for
exact matches, and not merely similarities.  Thus, query processing
will fail without exact matches. A mechanism will be needed for handling
"similar" statements with similar meanings.

Yes/No questions
----------------
"Did John throw a rock?"

 _subj(throw, John)
 _obj(throw, rock)
 HYP (throw, T)
 TRUTH-QUERY-FLAG (throw, T)
 TRUTH-QUERY-FLAG (rock, T)

Suggests a pattern-based approach:
  # IF _subj($vrb, $sb) ^
       _obj ($vrb, $ob) ^
       HYP($vrb)
    THEN answer "yes" if match ... 

Copula troubles:
----------------
Intensional/extensional inheritance causes problems for 
simple RelEx matching.

Yarn is a length of fibers.
_subj(be, yarn)
_obj(be, length)
tense(be, present)
of(length, fiber)
noun_number(length, singular)
noun_number(fiber, plural)
noun_number(yarn, uncountable)


Yarn is what?
(fails to parse in link-grammar)

What is yarn?

_subj(be, _$qVar)
_obj(be, yarn)
tense(be, present)
noun_number(yarn, uncountable)
COPULA-QUESTION-FLAG(yarn, T)
QUERY-TYPE(_$qVar, what)
noun_number(_$qVar, uncountable)

Note that the _subj and _obj are exchanged in the query, 
preventing a direct match.


Frame matching
--------------
The goal of working with frames rather than RelEx relations is
to escape from the strictures of the syntactic relations, and
move to a more semantic interpretation.

Yarn is a length of fibers.

^1_Categorization:Category(be,length)
^1_Existance:Entity(be,yarn)
^2_Inheritence:Quality(of,length)
^2_Inheritence:Inheritor(of,fiber)
^2_Inheritence:Group(of,fiber)
^2_Inheritence:Instance(of,length)
^1_Partitive:Subset(of,length)
^1_Partitive:Group(of,fiber)
^1_Part_whole:Part(of,length)
^1_Part_whole:Whole(of,fiber)
^1_Physical_entity:Constituents(of,fiber)
^1_Physical_entity:Entity(of,length)
^1_Categorization:Item(be,yarn)
^1_Temporal_colocation:Time(present,be)
^1_Temporal_colocation:Event(present,be)


What is yarn?

^1_Existance:Entity(be,_$qVar)
^1_Categorization:Category(be,yarn)
^1_Temporal_colocation:Time(present,be)
^1_Temporal_colocation:Event(present,be)
^1_Categorization:Item(be,_$qVar)
^1_Entity:Entity(what,_$qVar)
^1_Questioning:Message(what,_$qVar)
^1_Attributes:Entity(what,_$qVar)

Trim the question to remove references to questioning (as the answer
won't possess these). Remove the "what" part as well. The question
becomes:

^1_Categorization:Category(be,yarn)
^1_Temporal_colocation:Time(present,be)
^1_Temporal_colocation:Event(present,be)
^1_Categorization:Item(be,_$qVar)
^1_Existance:Entity(be,_$qVar)

The goal of pattern matching is to glue (i.e. pattern-match) 
the above five clauses onto the original assertion that 
"yarn is a length of fibers".

The glueing fails. I want to get the answer that $qVar == length.
But I cannot match up ^1_Existance:Entity(be,_$qVar)
to ^1_Existance:Entity(be,length) --

Next, I want to match up ^1_Categorization:Category(be,yarn),
(from the question) to ^1_Categorization:Item(be,yarn) (from
the answer) but I can't. 

This asymmetry of the question and its answer has me blocked.

The root problem seems to be that both

_subj(be, _$qVar)
_obj(be, yarn)
and
_obj(be, _$qVar)
_subj(be, yarn)

are valid forms for the question. The second one should be
more common, but its not the one being generated. There 
should probably be a frame rule to reverse this order.

============
Step back; look at frame matching in a simpler example.

John threw a ball. 
^1_Body_movement:Agent(throw,John)
^1_Cause_motion:Theme(throw,ball)
^1_Transitive_action:Agent(throw,John)
^1_Temporal_colocation:Event(past,throw)
^1_Temporal_colocation:Time(past,throw)
^1_Body_movement:Body_part(throw,ball)
^1_Cause_motion:Cause(throw,John)
^1_Transitive_action:Object(throw,ball)


What did John throw?

^1_Body_movement:Agent(throw,John)
^1_Cause_motion:Theme(throw,_$qVar)
^1_Transitive_action:Agent(throw,John)
^1_Entity:Entity(what,_$qVar)
^1_Questioning:Message(what,_$qVar)
^1_Attributes:Entity(what,_$qVar)
^1_Possibilites:Event(hyp,throw)
^1_Body_movement:Body_part(throw,_$qVar)
^1_Cause_motion:Cause(throw,John)
^1_Transitive_action:Object(throw,_$qVar)


Remove "Questioning" and "what".

^1_Body_movement:Agent(throw,John)          -- OK match
^1_Cause_motion:Theme(throw,_$qVar)         -- OK _$qVar == ball.
^1_Transitive_action:Agent(throw,John)      -- OK match
^1_Body_movement:Body_part(throw,_$qVar)    -- OK _$qVar == ball
^1_Cause_motion:Cause(throw,John)           -- OK match
^1_Transitive_action:Object(throw,_$qVar)   -- OK _$qVar == bal

This type of RelEx frame-level setup, query massaging 
and node matching is implemented in FrameQuery.cc, FrameQuery.h
It seems to work, but hasn't undergone strenuous testing.

Testing
-------
Some test corpus material, and test results:

====
Mike threw a rock.
John threw a ball and a screwdriver.
John ate a peach.
What did John throw?

RelEx matching provides two correct answers: 
_$qVar = ball, and _$qVar = screwdriver.
====

The book is red. What color is the book?
The color of the book is red. What color is the book?

Both of these fail on the simple pattern match.

====
Yarn is a length of fibers.
What is yarn?

Fails, as discussed above
====
Yarn is used to make cloth.
What is yarn used for?

_to-do(use, make)
_obj(use, yarn)
tense(use, present)
_subj(make, yarn)
_obj(make, cloth)
tense(make, infinitive)
HYP(make, T)
noun_number(cloth, uncountable)
noun_number(yarn, uncountable)

The assertion has a second parse, with "to" instead of "_to-do"

The question has three parses. The first is:

for(use, for)
_obj(use, yarn)
tense(use, present)
_amod(for, use)
_obj(for, _$qVar)
tense(for, present)
noun_number(yarn, uncountable)

All three parses contain _obj(use, yarn), and that's good.

Clearly, the pattern match here fails. All three parses contain
_obj(for, _$qVar) which cannot be matched to the original assertion.

All three contain either _amod(for, use) or _amod(for, be)
and these _amod's cannot be matched to the original assertion.

The frame situation is no better.  The assertion frame is:

^1_Transitive_action:Agent(make,yarn)
^1_Ingest_substance:Substance(use,yarn)
^1_Intentionally_create:Creator(make,yarn)
^1_Using:Instrument(use,yarn)
^1_Building:Created_entity(make,cloth)
^1_Causation:Affected(make,yarn)
^1_Arriving:Theme(make,yarn)
^1_Causation:Effect(make,cloth)
^1_Ingest_substance:Purpose(use,make)
^1_Cause_change:Cause(make,yarn)
^1_Possibilites:Event(hyp,make)
^1_Using:Purpose(use,make)
^1_Arriving:Goal(make,cloth)
^1_Temporal_colocation:Time(present,use)
^1_Temporal_colocation:Event(present,use)
^1_Transitive_action:Cause(verb,use)
^1_Transitive_action:Event(verb,make)
^1_Cooking_creation:Cook(make,yarn)
^1_Building:Agent(make,yarn)
^1_Cooking_creation:Produced_food(make,cloth)
^1_Intentionally_create:(make,cloth)
^1_Transitive_action:Object(make,cloth)
^1_Transitive_action:Object(use,yarn)
^1_Manufacturing:Product(make,cloth)
^1_Manufacturing:Factory(make,yarn)
^1_Cause_change:Entity(make,cloth)

The appearance of "cooking" is unexpected.  "Building" is odd 
Why not "manufacture" or "intentionally_create"?

"Ingest" is ... and odd way of saying that the yarn is used up...

  "What is yarn used for?"

Has the frameset below.  I've manually removed Questioning:Message, 
etc. as these are explicitly ignored for the question pattern match.
I've marked up each according to whether the frames echoed the
assertion:


^1_Using:Purpose(use,for)                         -- no match
^1_Ingest_substance:Substance(use,yarn)           == OK
^1_Using:Instrument(use,yarn)                     == OK
^1_Relative_time:Focal_occasion(for,for)          -- no match
^1_Relative_time:Landmark_occasion(for,use)       -- no match
^1_Temporal_colocation:Time(present,use)          == OK
^1_Temporal_colocation:Event(present,use)         == OK
^1_Temporal_colocation:Event(present_1,for)       -- no match
^1_Temporal_colocation:Time(present_1,for)        -- no match
^1_Ingest_substance:Purpose(use,for)              -- no match
^1_Transitive_action:Object(for,_$qVar)           -- no match
^1_Transitive_action:Object(use,yarn)             == OK

The word "for" seems to lie at the root of the problem.  
Lets try a different assertion:

  "Yarn is used for making cloth."

A clause by clause comparison shows that this doesn't match
the question any better then before. So pattern matching the
question fails here too.  

How similar are the two assertions? Lets compare RelEx output first;
ignoring tense, noun_number, which are correct, we have:

  "Yarn is used for making cloth."
_obj(use, yarn)
_amod(for, use)
_subj(make, for)
_obj(make, cloth)

  "Yarn is used to make cloth."
_to-do(use, make)
_obj(use, yarn)
_subj(make, yarn)
_obj(make, cloth)
HYP(make, T)

The only agreement is that both correctly state _obj(make, cloth)
and _obj(use, yarn)  However, _subj in both is confusing, and 
it seems hard to induce that the thing being used is also the 
input to the thing being made.

  "Yarn is used in making cloth."

_obj(use, yarn)
_amod(in, use)
_subj(make, in)
_obj(make, cloth)

  "Yarn is used in the making of cloth."

in(use, making)
_obj(use, yarn)
of(making, cloth)

The frame rules do not seem to introduce more commonality
between these; rather, they seem to diverge even more. Thus,
instead of the frames making things seem "more semantically
similar", they seem to do the opposite.

Now, some sort of fuzzy matching could be done here, since there
is clearly some non-zero overlap between all of these. Even more
so: it seems like the common parts of all of these ways of saying 
this capture the "underlying truth" the best. But without a large 
corpus, without lots of activation, and repeated similar but not 
identical restatements of the same ideas, I don't see any particularly
good ways of inducing that yarn is used to make cloth...


Triples matching
----------------
The nlp/triples directory contains code that generates semantic triples
from RelEx input.  So for example: "Bats are made of wood." results in
the semantic triple "made_of(wood, bat)", which is derived from the
raw relex dependencies:

    _obj (made_of, bat)
    _iobj (made_of, wood)

By contrast, consider the question: "What are bats made of?" It has a
very different RelEx dependency structure:

    _subj (be, bat)
    _obj (make, bat) 
    _obj (of, _$qVar) 
    of (be, of)

Because if this, performing simple patern matchin on the dependency 
structure will fail.

Lets try another: "The color of the book is red."
    _predadj (color, red)
    of (color, book) 
Which the semantic-triple rules convert to "color_of(book,red)"

By contrast, "what is the color of the book?" becomes

    _subj (be, _$qVar)
    _obj (be, color)
    of (color, book) 
    QUERY-TYPE (_$qVar, what)
    COPULA-QUESTION-FLAG (color, T)

which does have a semantic triple form: "color_of(book,what)"
Thus, for this case, question-answering by such normalized
"semantic triples" should work just fine.

By contrast, "what color is the book?" becomes

    _obj (be, book)
    _subj (be, color)
    QUERY-TYPE (_$qVar, what)
    QUERY-FLAG (color, T)

and the manner in which this should be converted into a "color_of"
question is unclear.


"you eat metal"  and " do you eat metal?"

What colour is the car?
 QUERY-FLAG (colour, T)
_subj (<<be>>, <<colour>>) 
_obj (<<be>>, <<car>>)
QUERY-TYPE (_$qVar, what)

note the borken parse -- missing connection to qVar.


References
----------
See:
http://www.cs.ualberta.ca/~lindek/papers.htm
Dekang Lin and Patrick Pantel. 2001. Discovery of Inference Rules for
Question Answering. Natural Language Engineering 7(4):343-360. [PDF][PS]

See also:
DIRT - Discovery of Inference Rules from Text, Dekang Lin and Patrick Pantel.


Query processing architectural issues:
--------------------------------------
Below are some architectural issues that have been given temporary
solutions, but need a better long-term solution.

-- How to distinguish rhetorical questions from queries that should
   be answered?  A block of input text may contain rhetorical questions,
   which should be ignored.

-- How should query processing be triggered?  What channel should be
   used for reply? Currently, all input to CogServer is via XML, which
   is parsed. If the parse is successful, no further processing is done.

   Query processing could be performed by a mind agent, but how should
   its activity be triggered, and what communications channel should
   it use to post a reply?


Diary and Notes
---------------
Use special token to trigger mind agent. Per Ben, "DialogManager",
there are four such tokens:
-- query
-- statement
-- command
-- interjection



