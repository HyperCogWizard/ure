
                   Language Learning
                   -----------------
             Linas Vepstas December 2013

Current project, under construction.

Steps:
1) Install the 'any' language for link-grammar
2) Set up and configure postgres, as described in 
   opencog/persist/sql/README
3) createdb learn-pairs
   cat atom.sql | psql learn-pairs
   Edit and setup ~/.odbc.ini
4) Start the cogserver

5) Copy the RelEx opencog-server.sh shell script to some other name.
   Edit it, and replace the final line with this:
   java $VM_OPTS $RELEX_OPTS $CLASSPATH relex.Server --lang any -n 999 --link --port 4444
   Run the new shell script.

   The above tells relex to use the 'any' language, and return up to 999
   different parses.

6) cat ./link-pipeline.scm |netcat localhost 17001

   Better idea: edit build/lib/opencog.conf and add nlp/learn/link-pipeline.scm
   to the list

7) telnet localhost 17001
   opencog> sql-open learn-pairs linas asdf
   opencog> scm
   guile> (observe-text "this is a test")

   Better yet:
   echo -e "scm\n (observe-text \"this is a another test\")" |nc localhost 17001
   echo -e "scm\n (observe-text \"Bernstein () (1876\")" |nc localhost 17001
   echo -e "scm\n (observe-text \"Lietuvos žydų kilmės žurnalistas\")" |nc localhost 17001

8) psql learn-pairs
   learn-pairs=# SELECT * from atoms;

   The above shows that the database now contains word-counts for
   pair-wise linkages for the above sentences.

9) Get ready to feed it text.  For example, parse wikipedia.
   a) Download lang-wiki-pages-articles for some lanugage lang.
   b) See 'Wikipedia processing' section below. Do the stuff there.
   c) See the section 'Sentence Splitting'. Set that up.
   d) See Wikipedia 'Processing, part II' below.

10) Do the stuff in section "Mutual Information".

That's all for now, folks!


TODO
----
* Remove the NLP_HACK from persist/sql/PersistModule.cc
* Optimize the scm scripts to not pound the database so hard.
  (How? Lots of word pushes, but really, there's not much repeat
  traffic ...)
* Investigate crash: while writing truuth value, stv_confidence 
  was pure virtual (circa line 920 of AtomStorage.cc)
  -- this only happened when I allowed the atomspace to get
     bigger, by not agressively deleting old sentences!
     I don't see what this has to do with it ...

DONE
----
* Make sure that link-parser fully randomizes linkage choices for
  long sentences. Done. See the rand_state and predictable_rand
  flags in version 4.8.3.
* Need a mutli-language sentence splitter; the maxent splitter only
  works for English sentences.  Probably something simple will do ...
  Done: copied one from moses-smt and put it into a relex subdirectory
* Raw psql seems not to be utf8.  Were the tables created wrong,
  or is this a client bug? 
  Fixed: turned out to be a guile-related bug.
* Handle database write asynchronously: have a thread collect
  up the atoms, and write them. Maybe even multiple threads.
  This is OK due to the atomptr design.
  Done.  Not just one, but multiple writers.
* Can we start a guile thread for each incoming sentence?
  Threading will require more subtle sentence cleanup.
  No. The main guile bug, opened 5 years ago, is still not fixed.
* Table counts need to be more than 32-bit. Looks like the 
  any-language goes hog-wild and creates huge counts for just
  one sentence ... is this desired?  Anyway, they'll overflow...
  Done. atoms.sql now uses bigint 64-bit uuids.


Notes:
------
The relex parse is extreely fast, the cogserver part is extremely slow ... 
Non-linear, even.
4-word sentence: 53 parses, 3-4 seconds elapsed

5-word sentence, 373 parses,
32 second elapsed,
postgres runs at 36% cpu
cogserver runs at 56% cpu for a while then drops to 26% cpu

8-word sentence, max of 999 parses,
2 minutes elapsed.  Same CPU usage as above...


Wikipedia processing
--------------------
Do the following:
    createdb fr_pairs lt_pairs pl_pairs simple_pairs
    cat opencog/persist/sql/atom.sql | psql
    vi .odbc.ini  learn-fr learn-lt learn-pl learn-simple

cleanup files in relex: src/perl

    cd /storage/wikipedia

    mkdir wiki-stripped
    time cat blahwiki.xml.bz2 |bunzip2 | /home/linas/src/relex/src/perl/wiki-scrub.pl
    cd wiki-stripped
    find |wc
    /home/linas/src/relex/src/perl/wiki-clean.sh
    find |wc
    cd ..
    mkdir alpha-pages
    cd alpha-pages
    /home/linas/src/relex/src/perl/wiki-alpha.sh

simple:
find |wc     gives 131434 total files
find |wc     gives 98825 files after cat/template removal.

lt:
7 mins to unpack
find |wc gives 190364 total files
find |wc gives 161463 after cat/template removal

pl:
1 hour to unpack (15 minutes each part)
find | wc gives 1097612 articles
52K are categories
35K are templates
find |wc gives 1007670 files after cat/template removal

fr:
3 hours to unpack (25-60 minutes per glob)
find |wc gives 1764813 articles
214K are categories
55K are templates
find |wc gives 1452739 files after cat/template removal


Sentence Splitting
------------------
Needs to be multi-language
Maybe NLTK ? .. Not today, but you can experiment.
Long-term goal is to have opencog know how to sentence-split itself,
but this is secondary prioruity.

Here's a simple one, easy-to-use:
https://github.com/moses-smt/mosesdecoder/tree/master/scripts/share/nonbreaking_prefixes
Has french, polish, latvian
its LGPL
Copy this over into relex, for now. (Its in the RelEx sources)

cat /storage/wikipedia/ltwiki-20131216-pages-articles/alpha-pages/A/./Akiduobė | /home/linas/src/relex/src/split-sentences/split-sentences.pl -l lt > x

A question mark or exclamation mark always ends a sentence.  A period followed by an
upper-case letter generally ends a sentence, but there are a number of exceptions.  For
example, if the period is part of an abbreviated title ("Mr.", "Gen.", ...), it does not end a
sentence.  A period following a single capitalized letter is assumed to be a person's initial,
and is not considered the end of a sentence.

(setlocale LC_ALL "")
 (display "Ćićolina")


Wikipedia Parsing, part II
--------------------------
Now, build some working directories (so that we don't mess up
alpha-pages after all that work)

    cp -pr alpha-pages beta-pages

* Copy scripts from opencog/nlp/learn/misc-scripts into working dir.
* Modify as needed for chosen language. 
* Make sure the scripts point at the RelEx parse server.
* Copy opencog-lang.conf to opencog build dir.
  Start cogserver -c opencog-lang.conf 
* Go back to the wikipedia dir, and run ./wiki-ss-lang.sh
* Wait a few days.
* Be sure to psql lang_pairs -c "VACUUM ANALYZE;" every 4 hours or so,
  as otherwise, performance craters.  Be sure to perform the postgres
  tuning recommendations fount in postgress peformance wiis, or in
  the opencog/persist/sql/README file.  See also 'Performance' section
  below.

Some handy SQL commands:
SELECT count(uuid) FROM Atoms;
select count(uuid) from  atoms where type = 77;

type 77 is WordNode for me; verify with
SELECT * FROM Typecodes;


Mutual Information
------------------
Compute mutual information, manually, by runnning these scripts:
compute-mi.scm Go to the bottom, and run (do-em-all)

Example stats and performance:
current fr_pairs db has 16785 words and 177960 pairs.

This takes 17K + 2x 178K = 370K total atoms loaded.
These load up in 10 seconds-ish or so.

RSS for cogserver: 436MB, holding approx 370K atoms
So this is about 1.2KB per atom, all included. Atoms are a bit fat...
... loading all pairs is very manageable even for modest-sized machines.

Wild-card pair-count is taking about 30 per minute, so total time will
be  17K minutes/30 = 580 minutes = 10 hours!  Oy vey...
This is utterly single-threaded...

all-pair count was fast.

Final mi compute: 2910 in 73 mins == 40 per minute ... again, pathetic.
Will take 7 hours (420 minutes) to complete ...

Wooot!  After numerous fixes, to avoid the old atomspace brain-damage
we are now up to about 250 per minute on wild-card, that's an 8x speedup!
Although thats still slower than e.g. a perl script ... oh well,
price of generality.  Avoiding the pattern matcher would go a long way.


Next step: minimum spanning tree parsing
----------------------------------------
Minimum spanning tree code is in mst-parser.scm

he MST parser discovers the minimum spanning tree that connects the
words together in a sentence, the mutual information between word-pairs
is used as the link-cost. (actually, we're maximizing, not minimizing MI).

The algorithm works. The next step is to store the results, and start
clustering them to auto-discover link-grammar links.


Some typical entropies for word-pairs
-------------------------------------
(WordNode "famille") entropy H=11.185

H(*, famille) = 11.195548 
H(famille, *) = 11.174561

MI(et, famille) = -5.2777815
H(et, *) = 5.5696678
P(et, *) = 0.021055372363972875

thus: 
H(et, famille) = -MI(et, famille) + H(famille, *) + H(et, *) = 22.0220103
P(et, famille) = 2.348087815164205e-7

MI(de, famille) = 2.1422486
H(de, *) = 4.3749881
P(de, *) = 0.04819448582223504

H(de, famille) = -2.1422486 + 4.3749881 + 11.195548 = 13.4282875
P(de, famille) = 9.071574511509601e-5

P(de+et. *) = 0.06924985818620791
H(de+et, *) = 3.8520450730427047

P(de+et, famille) = 9.095055389661243e-5
H(de+et, famille) = 13.424558050397735
MI(de+et, famille) = 1.6230350226449701

So  MI(et, famille) < MI(de+et, famille) < MI(de, famille)
       -5.2777815   <   1.6230350226     <   2.1422486

By contrast, the arithmetic average is:
(MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille)) /
    (P(de, famille) + P(et, famille))
  = 2.1230921666199825

Change in entropy:
MI(de, famille) * P(de, famille) + MI(et, famille) * P(et, famille) =  0.0012169

MI(de+et, famille) * P(de+et, famille) = 1.476159343e-4

Oh, wait ... 
H(de, famille) * P(de, famille) + H(et, famille) * P(et, famille) =  0.001223328

H(de+et, famille) * P(de+et, famille) = 0.00122097099

Change in entropy = 0.00122097099 - 0.001223328 = -2.35701e-6

-------
H(de) = 4.3808608
H(et) = 5.5862331

P(de) = 0.04799870191172842
P(et) = 0.02081499323761464
P(de+et) = 0.06881369514934306
H(de+et) = 3.8611604742976153  = -log_2 (P(de)+P(et))
By contrast, the weighted average is

(P(de)*H(de) + P(et)*H(et)) /(P(de) + P(et)) = 4.745465784790553

Combinations:
  P(de+et)*H(de+et) = 0.2657007
  P(de)*H(de) + P(et)*H(et) = 0.32655303

The change in entropy, from forming a union, is:
  P(de+et)*H(de+et) - P(de)*H(de) - P(et)*H(et) = -0.060852316

Recap: Delta(de+et) = -0.060852316
       Delta(de+et, famille) = -2.35701e-6

Entropy increases (strongly) if word-pair merged, words are separated,

-------

MI(d'une, famille) = 5.230504
H(d'une, *) = 9.792551

H(la) = 5.6536283
H(la, *) = 5.5858526

sa  
est
de 
H(d'une) = 9.7960119
H(un) = 7.1578913
H(et) = 5.5862331

-----
repeat, for vielle+nouveaux
H(nouveaux) = 14.28815
P(nouveaux) = 4.998483553100357e-5

H(vieille) = 16.16037
P(vieille) = 1.365349e-5

P(nouveaux+vieille) = 6.363833e-5
H(nouveaux+vieille) = 13.93974
P(nouveaux+vieille)*H(nouveaux+vieille) = 8.87102088-4

P(nouveaux)*H(nouveaux) + P(vieille)*H(vieille) = 9.3483638e-4

Change in entropy is diff of the two: -4.7734297e-5


-----
repeat, for vielle+nouveaux
H(*, famille) = 11.195548 

H(nouveaux, *) = 13.974219
P(nouveaux, *) = 6.213565989765264e-5
MI(nouveaux, famille) = 5.2966957
H(nouveaux, famille) = 19.8730713
P(nouveaux, famille) = 1.0413804797188067e-6

H(vieille, *) = 15.998603
P(vieille, *) = 1.5273571710064995e-5
MI(vieille, famille) = 10.195547
H(vieille, famille) = 16.998604
P(vieille, famille) = 7.636780561617735e-6

P(vieille+nouveaux, famille) = 8.678161041336542e-6
H(vieille+nouveaux, famille) = 16.814179210712517

P(vieille+nouveaux, *) = 7.740923160771763e-5
H(vieille+nouveaux, *) = 13.657134846045357

MI(vieille+nouveaux, famille) = 8.038503635332841

so MI(nouveaux, famille) < MI(vieille+nouveaux, famille) < MI(vieille, famille)
         5.2966957       <         8.038503635332841     <       10.195547

Change in entropy:
P(nouveaux, famille)*H(nouveaux, famille)  + P(vieille, famille)*H(vieille, famille)
    = 1.505100371e-4

P(vieille+nouveaux, famille) * H(vieille+nouveaux, famille) = 1.459161549e-4

Change = 1.459161549e-4 - 1.505100371e-4 = -4.5938821e-6

To recap: Delta(vieille+nouveaux) = -4.7734297e-5
               reduces the entropy more than
          Delta(vieille+nouveaux, famille) = -4.5938821e-6

i.e. entropy increses if the word-pairs are merged, the words are separated.


---------------

Total entropy:
(fold + 0 (map (lambda (atom)
	(let ((ent (tv-conf (cog-tv atom))))
	(* ent (exp (* (- ent) (log 2))))))
	(cog-get-atoms 'WordNode)))

gives: 7.2199274514956




Crash
-----
pure virtual method called
terminate called without an active exception
[2013-12-27 03:13:59:571] [ERROR] Caught signal 6 (Aborted) on thread

        11: AtomStorage.cc:920
opencog::AtomStorage::do_store_single_atom(std::shared_ptr<opencog::Atom>, int)
STMTF("stv_confidence", tv->getConfidence());

Again: 
        9: ??:0 __cxa_pure_virtual()
        10: AtomStorage.cc:929    opencog::AtomStorage::do_store_single_atom(std
::shared_ptr<opencog::Atom>, int)
        11: AtomStorage.cc:802
opencog::AtomStorage::do_store_atom(std::shared_ptr<opencog::Atom>)

this time, line 929 is:
926        // Store the truth value
927        TruthValuePtr tv = atom->getTruthValue();
928        TruthValueType tvt = NULL_TRUTH_VALUE;
929        if (tv) tvt = tv->getType();

again... 26 December, no stack-trace, but got "called pure virtual" error msg.
again... 27 December line 939: STMTF("stv_confidence", tv->getConfidence());
again... 27 Dec, 929        if (tv) tvt = tv->getType();
again... 28 Dec, didn't get a stack...
again... 28 Dec, line 953:  STMTF("stv_mean", tv->getMean()); on count tv
again... 30 Dec, line 954:  as above ... 
again... 30 Dec, line 957:  tv->getMean() as above, now unwrapped.
again... 31 Dec, this time typeid caught it.
again... 31 Dec, this time typeid prints: N7opencog10TruthValueE which is
          crazy, and adress 0x802ef558 which is sane,
again... 31 Dec, tv-cnt=2 atom-cnt=504697
again... 31 Dec, line 937: if (tv) tvt = tv->getType();
again... 1 Jan,  addr=0x2b85a3c38 atom=ANY tv-cnt=1 atom-cnt=4050379  WHERE uuid = 185;

Multi-issues:
1) stv_count maxed at 16777216
2) same bad tv ptr on two different threads.... two different atoms....
   ListLink
      WordNode fiksuojami
      WordNode 50

alter table atoms alter column stv_count type double precision;

Ohhhh .. I get it ... getTV and setTV are not locked.
Another thread is doing setTV even while this thread is doing getTV.  The
use-count pointer drops to zero, destructor runs, then magically, use-count
goes back up to one...  basically, shared_ptr's are not thread-safe.

OK, so fixed all the above, and it should work, right?

again... 1 Jan, even after the fixes... WTF???
again... 2 Jan, even after the fixes... WTF???
again... 3 Jan, even after the fixes... WTF???
       except this time, it wasn't (LinkGrammarRelationshipNode "ANY")
       but instead some link ... 
       N7opencog10TruthValueE addr=0x28ecc9658 tv-cnt=2 atom-cnt=247 atom-type=27
       name=(not-node) cols=UPDATE Atoms SET  vals= coda= WHERE uuid = 175787728; 
       Very recent atom being added.
again... 4 Jan, even after the fixes... WTF???
       Its the (LinkGrammarRelationshipNode "ANY") as usual.



Crash 2
-------
All new crash. Note:
1) This is with the old scheme singleton-instance.
2) This one received when defining a function with an unbound variable
   in it.  It should have just been a standard stack trace, but wasn't.
3) Sometimes, when doing this, and parser is running, I get 

[2013-12-26 22:50:57:680] [ERROR] Caught signal 11 (Segmentation fault) on thread 47486186035520
        Stack Trace:
        2: CogServerMain.cc:92  _Z7sighandi()
        3: sigaction.c:0        __restore_rt()
        4: ??:0   std::string::append(std::string const&)
        5: basic_string.h:290     std::string::_M_data() const
        6: SchemeEval.cc:381      opencog::SchemeEval::c_wrap_eval(void*)
line 381 is  self->answer = self->do_eval(*(self->pexpr));
So maybe p is null, or self->pexpr is null..
        7: continuations.c:511  c_body()
        8: vm-i-system.c:855    vm_regular_engine()
        9: eval.c:508   scm_call_4()
        10: continuations.c:456 scm_i_with_continuation_barrier()
        11: continuations.c:550 scm_c_with_continuation_barrier()
        12: pthread_support.c:1272      GC_call_with_gc_active()
        13: threads.c:937       with_guile_and_parent()
        14: misc.c:1835 GC_call_with_stack_base()
        15: threads.c:959       scm_with_guile()
        16: SchemeEval.cc:372     opencog::SchemeEval::eval(std::string const&)
        17: basic_string.h:536  ~basic_string()
        18: GenericShell.cc:157   opencog::GenericShell::eval(std::string const&, opencog::ConsoleSocket*)
        19: ConsoleSocket.cc:232          opencog::ConsoleSocket::OnLine(std::string const&)
        20: basic_string.h:536  ~basic_string()
        21: thread.cpp:0        thread_proxy()
        22: pthread_create.c:0  start_thread()
        23: ??:0        __clone()


Crash 3
-------
30 Dec 2013:
Too many heap sections: Increase MAXHINCR or MAX_HEAP_SECTS
  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND            
24743 linas     15   0 30.1g  21g 8340 S  0.0 30.1 448:50.56 cogserver          

31 Dec: again...
1 Jan: again...
  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND            
14550 linas     15   0 23.9g  16g 8344 S  0.0 23.1 337:22.45 cogserver

bdwgc ./configure --with-large-heap=yes
#ifndef LARGE_CONFIG
#define LARGE_CONFIG
#endif

Irritation 4
------------
Occasionally see this: 

In ice-9/boot-9.scm:
 157: 6 [catch #t #<catch-closure 294a63c0> ...]

   ?: 3 [apply-smob/1 #<catch-closure 294a62a0>]
   ?: 2 [call-with-input-string "(observe-text \"Cheopso piramid\u0117, arba Did
\u017eioji Gizos piramid\u0117 \u2013 Egipto piramid\u0117, faraono Cheopso kapa
s.\")\n" ...]
In ice-9/boot-9.scm:
 102: 1 [#<procedure 193e5c80 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encodi
ng-error ...]

-----------
another:
 "(observe-text \"Andrzej G\u0105siorowski i Krzysztof Steyer, Tajna Organizacja Wojskowa Gryf
Pomorski, Polnord Wydawnictwo Oskar, Gda\u0144sk 2010 r.\")\n" .

Andrzej Gąsiorowski i Krzysztof Steyer, Tajna Organizacja Wojskowa Gryf Pomorski, Polnord
Wydawnictwo Oskar, Gdańsk 2010 r.

In ice-9/boot-9.scm:
 102: 1 [#<procedure 3db5240 at ice-9/boot-9.scm:97:6 (thrown-k . args)> encoding-error ...]
In unknown file:
   ?: 0 [apply-smob/1 #<catch-closure 3be53c40> encoding-error ...]

ABORT: encoding-error



Performance
-----------
Performance seems to suck: 
-- two parsers, each takes maybe 4% cpu time total. Load avg of about 0.03
-- each parser runs 4 async write threads pushing atoms to postgres. 
   each one complains about it taking too long to flush the write queues.
-- postmaster is running 10 threads, load-avg of about 2.00  so about
   2 cpu's at 100%
-- vmstat shows 500 blks per second written. This is low...
-- top shows maybe 0.2% wait state. So its not disk-bound.
-- what is taking so long?

So, take a tcpdump:
-- a typical tcpdump packet:
   UPDATE Atoms SET tv_type = 2, stv_mean = 0 , stv_confidence = 0, stv_count = 54036 WHERE uuid = 367785;
   its maybe 226 bytes long.
-- this gets one response from server, about 96 bytes long.
-- then one more req, one more repsonse, seems to be a 'were'done' mesg
   or something ...  which I guess is due to SQLFreeHandle(SQL_HANDLE_STMT ???
-- time delta in seconds, of tcpdump of traffic packets, between update, and 
   response from server:
   0.0006  0.0002 0.0002 0.0002 0.028 (yow!!) 0.001 0.0002

-- so it looks like about every 8-10 packets are replied to failry quick,
   then there's one that takes 0.025 seconds to reply.... stairsteps in
   response time like this all the way through the capture.

Wild guess:
-- Hmm ... this seems to be related to the commit delay in postgresql.conf
   Change commit_delay to 1 second
   change wal_bufers to 32MB since its mostly update traffic.
   change checkpoint_segments to 32 (each one takes up 16MB of disk space.)

-- Making these changes has no obvious effect ... bummer.

I don't get it; performance sucks and I don't see why.  Or rather: postmaster
is chewing up vast amounts of cpu time for no apparent reason...


select * from pg_stat_user_tables;
select * from pg_stat_all_tables;
select * from pg_statio_user_tables;
select * from pg_database;

pg_stat_user_indexes
pg_stat_all_indexes

select * from pg_catalog.pg_stat_activity;
select * from pg_catalog.pg_locks;


-- WOW!!!   VACUUM ANALYZE; had a huge effect!!

-- vacuum tells em to do following:
   change max_fsm_pages to 600K
   chage max_fsm_relations to 10K

Anyway ... performance measured as of 27 Dec 2013:

Takes about 105 millisecs to clear 90 eval-links from the write-back
queues. This each eval-link is 5 atoms (eval, defind, list, word, word)
so this works out to 5*90 atoms /0.105 seconds = 4.3KAtoms/sec 
which is still pretty pathetic... 


Misc 
----
Change load-atoms to return a count?

gdb:
handle SIGPWR nostop noprint
handle SIGXCPU nostop noprint


How about using a reader-writer lock?
----------------------------------

boost::shared_lock  for reading,
unique_lock for writing ... 

upgrade_lock<shared_mutex> lock(workerAccess); 
        upgrade_to_unique_lock<shared_mutex> uniqueLock(lock);



shared_mutex 
write uses:  unique_lock<shared_mutex>
readers use shared_lock<shared_mutex>

writer does:

  // get upgradable access
  boost::upgrade_lock<boost::shared_mutex> lock(_access);

  // get exclusive access
  boost::upgrade_to_unique_lock<boost::shared_mutex> uniqueLock(lock);
  // now we have exclusive access
}

am using boost1.49 on cray

------------------------------------------


