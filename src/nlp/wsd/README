
                      Word Sense Disambiguation
                      -------------------------
                           Working Notes 
                      Linas Vepstas April 2008


This document contains working notes for performing word-sense
disambiguation within the framework of OpenCog. The first section
reviews the mapping of WordNet ideas onto OpenCog structures. The
second section discusses how various different ideas of "word-sense"
are associated with a parsed sentence, and the algorithms that can
be used to perform the actual disambiguation. 

Preliminary comments
--------------------
Per current conversations with Ben Goertzel, new links introduced in
the text below should be understodd to be a kind of predicate. That is,
a link of the form

   <BlahLink>
      <Node name="AAA" />
      <Node name="BBB" />
   </BlahLink>

is to be understood to be equivalent to 

   <EvaluationLink>
     <PredicateNode name="Blah"/>
     <ListLink>
       <Node name="AAA" />
       <Node name="BBB" />
     </ListLink>
   </EvaluationLink>

Although opencog does not currently handle this equivalance in a 
transparent fashion, this is the long-term intention.  Thus, the text
below will mix these two different representation forms.

Word senses
-----------
Associated with every word is a list of possible senses.
Consider the word "bark", thus:

    <WordNode name="bark" />   -- a word.

Wordnet netries are not necessarily single words; more properly, the
should be called "lexical units": they can be single words,
collocations, idioms, and so on.

A "word sense" is a collection of information about a single semantic
sense of a word. This bag includes information about the part-of-speech
(noun, verb, etc.), example usages, pointers to synonym sets (synsets),
hyponyms, hypernyms, etc. A specific "word sense" will be tagged with a
unique identifier, which is then used to reference that sense.

In the OpenCog type hierarchy, (type.script)
   WORD_SENSE_LINK <- ASSOCIATIVE_LINK

A WordSenseLink couples the tag that will stand for the word sense,
to the word itself:

   <WordSenseLink>
      <WordNode name="bark" />
      <ConceptNode name="bark_sense_23" />
   </WordSenseLink>

The tag value "bark_sense_23" has no particular meaning in itself,
it is just some unique string used to identify the sense.

Word senses can be crudely categorized according to part-of-speech.
Thus, for example:

   <PartOfSpeechLink>
      <ConceptNode name="bark_sense_23" />
      <PartOfSpeechNode name="noun" />
   </PartOfSpeechLink>

The above introduces a new node type "PartOfSpeechNode", and a new link
type: "PartOfSpeechLink".  Other possible properties might include gender,
entity tags, and so on. 

Each of these nodes and links are assigned a truth value of 1 and a
confidence of 1. Non-unit truth-values & confidence will be used when 
relating word senses to the use of actual words in sentences. Here, 
however, the goal is to state "this word does have this as one possible
meaning, no matter how rarely used this meaning might be."

[[Some word senses are more common than others. For example, archaic
word senses tend to be rarely encountered ... except when one is 
reading an archaic text. This is addressed in a section below.]]

In WordNet, word senses are commonly established by giving "sister
terms" (a WordNet concept, equivalent to "synonyms for word senses")
as well as hypernyms, hyponyms, part holonyms, etc. A sister term,
hypernym, hyponym, etc. are thus links. Since some sister terms are 
closer than others, these links will have non-unit truth values. 

Hypernyms and hyponyms will be mapped to inheritance links.

Details of the actual mapping used are given in greater detail 
in the wordnet-import/README file.

Sentences to meanings
---------------------
Consider two sentences: "The loud bark woke me up." or "The rough bark
cut my finger." The goal of word-sense disambiguation is two-fold:
associate to all possible meanings of the word "bark", and then, rank
the possible meanings from most likely to least.

There are six related ideas of a word, and its sense, that need to 
be distinguished. These are:

1) The "word instance", as it occurs in the sentence. Word instances
   may be mis-spelled, and thus it may be ambiguous as to which 
   actual dictionary word was intended.

2) The "dictionary word", the word as it would be in the standard
   lexicon of a language: a word in its proper spelling, and with
   morphological inflections stripped away: the lemmatized or root form
   of the word.

   Dictionary-words are represented by WordNode.

3) The "word sense", a particular sense of a word, as it might appear
   in a dictionary, or in WordNet. For example, "bark" in the sense of
   "tree bark". A word-sense exists independently of a particular
   sentence or text.

4) The "word parse", a linkage of a word-instance to a particular
   sentence parse. In the course of parsing, syntactical information
   is assigned to the word-parse: the word parse is identified by a
   a part-of-speech, a tense, a number, as a subject or object. The
   assignment of these properties may vary from parse to parse.
   For any one given sentence, there may be as many different
   word-parses as there are sentence parses. Typically, most sentence
   parses will assign the same word properties to each word-parse;
   these can then be collapsed into one, with the realization that the
   identification of the word-parse is unambiguous across all of the
   different parses.

   Note that a word-parse is also an assignment of a word-instance to
   a dictionary-word. Although rare, there are words in the English 
   language which have multiple different base forms; for example,
   "axes" is the plural form of both "axe" and "axis". Two different
   parses might then assign different base-forms to the word-instance.
   This may also occur if a word was mis-spelled: for instance, using 
   "there" instead of "their". An incorrect parse would link the 
   mis-spelled word-instance to the spelled-the-same but-wrong 
   dictionary-word; the correct parse would link the word-instance 
   to its correct spelling.

5) The "word concept", the linkage of a word sense to a word parse.
   The "word concept" is distinguished from the "word sense", as being 
   a particular word sense within context of the sentence in which it 
   appears. Word-concepts may be uncertain or ambiguous: the meaning of
   any particular word-instance, as it appears in a sentence, may not be
   entirely clear. Initially, a link is created between a word-parse and
   every possible appropriate word-sense (based on the part-of-speech
   assignment, etc.). Each link is assigned equal weight. The goal of 
   word-sense disambiguation is to (strongly) prefer one particular
   word-concept over another.

6) Finally, there is a "text-sense", which is the disembodied concept
   that a word instance is referring to, across the entire text. For
   example, in the text "Mary looked at the glass. She picked it up."
   the word-instances "she" and "Mary" both refer to the same
   text-sense, even though they are represented by completely different
   surface words. The relation of text-senses to a text is also
   probabilistic: there is a non-zero probability that "she" does 
   not actually refer to Mary.  At the basic level, a text-sense 
   is a link between a node "Mary the abstract concept" and various 
   word-concepts. 

   Thus, for example, the word "Mary" has multiple possible word-senses:
   "Mary" may be a human being, but also, "Mary" may be a dog or other 
   animal, or "Mary" may be a statue of the the Virgin Mary.  The act of 
   word-sense disambiguation will assign a probability to the 
   word-concepts binding "Mary" to each of these possible word-senses.
   Then, the likelihood that "she" refers to "Mary" depends on the 
   likelihood that "Mary" refers to a statue of the Virgin Mary, as
   statues cannot pick things up. 

   (Incidentally, this gives some mathematical insight into humour,
   poetry, and "magical" and fantasy literature: in the normal world,
   a statue cannot pick things up. But a well-directed joke can force
   one to conclude that it must have: an impossibility, and therefore
   funny. A "well-directed" joke does this by making sure that there
   is no ambiguity in either the parse, or in the associated
   word-senses, so that there is no question, upon arriving at the punch
   line, of what it implies, however absurd that may be.)

To illustrate the above:

Consider the example text: "We stood in the yard.  As she walked up to
me, I laid my hand on the old oak.  Its rough bark cut my finger."
This sentence contains the word-instance "bark". The word-parse of
"bark" is unambiguous: it is a noun.  From WordNet, we know that 
"bark-the-noun" has several different word-senses: it may be tree-bark,
or it may be a type of watercraft.  The word-concept would be 
a linkage from the word-parse "bark-the-noun" to the sense of "tree-bark";
the goal of word-sense disambiguation is to give this particular 
word-concept a high probability and a high confidence, as compared to
"bark-the-watercraft".  Lastly, there is the text-sense. In this text, 
the text-concept for "bark" can be deduced to be "the bark of an oak
tree in a particular yard in which there are two people".

Network structure
-----------------
The goal of the OpenCog network structure given below is to properly 
capture the above-described relationships, inserting probabilities 
in all of the right places, and allowing an algorithm to walk the
network of connections and assign the highest probabilities to the most
likely interpretations.

The basic algorithmic components are as follows:
A) Import WordNet data to establish "dictionary-words" and "word-senses",
   as defined above.
B) Use the Link-Grammar+RelEx infrastructure to identify 
   "word-instances", and link them to "word-parses", and to assign
   initial probabilities ranking alternative parses.
C) Use Radu Mihalcea's word-sense-disambiguation algorithms to compute
   initial probability assignments for "word-concepts".
D) Use anaphora resolution algorithms to assign initial probabilities
   to text-senses.
E) Use "common-sense reasoning" to refine probabilities for text-senses.
   A given text-sense probability can act as feedback for the
   likelihood of a given parse being correct, and so the initial
   probabilities assigned in steps B,C,D are then iterated upon,
   in an effort to find convergence.


Word-parse meaning
------------------
This section elaborates on point 4) above.

Word-parses assign syntactic meaning to a word, within the context of a
parse. Thus, for example, the goal is to express "in parse 2, the word
'bark' has part-of-speech 'noun'". Such associations are required 
because, in parse 3, the word 'bark' might be a verb.

Mathematically, this is a quadruple: (parse2, 'bark', part-of-speech, 
'noun'). Most PLN structures are oriented around triples, and so this
section explores how to map this quadruple into a set of predicate 
triples.

This suggests the structure below. Each parse of a sentence is assigned
an initial parse ranking, and so:

   <ConceptNode name="parse_1" strength=0.9 confidence=0.6/>
   <ConceptNode name="parse_2" strength=0.8 confidence=0.5/>
   <ConceptNode name="parse_3" strength=0.6 confidence=0.3/>

Each word in each parse of a sentence gets a unique concept node
assignement. This is in order to allow the same word-instance be
associated to different values in different parses. Suppose, for
example, that, in two different parses of the same sentence, the word
"bark" is identified as a noun, or as a verb. Thus, one has distinct
nodes:

   <!-- Identify bark_144 as being parse instance 2 -->
   <EvaluationLink>
     <PredicateNode name="ParseInstance"/>
     <ListLink>
       <ConceptNode name="bark_144" />
       <ConceptNode name="parse_2" />
     </ListLink>
   </EvaluationLink>

   <!-- Identify bark_169 as being parse instance 3 -->
   <EvaluationLink>
     <PredicateNode name="ParseInstance"/>
     <ListLink>
       <ConceptNode name="bark_169" />
       <ConceptNode name="parse_3" />
     </ListLink>
   </EvaluationLink>

As promised by the example, one of these identifies "bark" as a 
noun, the other as verb:

   <!-- In parse 2, which has bark_144, it is a noun -->
   <EvaluationLink>
     <PredicateNode name="PartOfSpeech"/>
     <ListLink>
       <ConceptNode name="bark_144" />
       <ConceptNode name="noun" />
     </ListLink>
   </EvaluationLink>

   <!-- In parse 3, which has bark_169, it is a verb -->
   <EvaluationLink>
     <PredicateNode name="PartOfSpeech"/>
     <ListLink>
       <ConceptNode name="bark_169" />
       <ConceptNode name="verb" />
     </ListLink>
   </EvaluationLink>

It should be made clear that both concepts "bark_144" and "bark_169"
refer to the same word in the same sentence.

   <!-- in parse 2, the word "bark" is associated with bark_144 -->
   <ReferenceLink>
      <WordNode name="#bark">
      <ConceptNode name="bark_144" />
   </ReferenceLink>

   <!-- in parse 3, the word "bark" is associated with bark_169 -->
   <ReferenceLink>
      <WordNode name="#bark">
      <ConceptNode name="bark_169" />
   </ReferenceLink>

In the above, the associated WordNode is meant to be the non-lemmatized
form of the word, since, in principle (though extremely rare in
practice), the lemmatization might be different for different parses.
(By "lemmatized form", we mean the infinitive form of a verb, the 
singular of a plural noun, etc.)

[[In the above, "bark_144" and "bark_169" were identified as 
ConceptNodes. However, for clarity, it migh be convenient to
identify these as WordInstanceNodes?]]

[[In the above, a collection of triples were used to specify the
quadruple relation (parse 2, 'bark', part-of-speech, 'noun'), which can
be read as "in the context of parse 2, the word 'bark' is a noun". This
suggests that perhaps ContextLink could be used?]]

If all parses agree on the properties of a given word, it would
be advantageous to collapse the proliferation of word-instances
down to one or two, as early in the processing as possible.

[[-----
The current RelEx output is as follows, but this is at odds with the 
discussion above. It will be abandoned shortly!?

   <!-- link the word-instance to the "text-sense" bark_1 -->
   <InheritanceLink>
      <WordInstanceNode name="bark_1234" />
      <ConceptNode name="bark_1" />
   </InheritanceLink>

   <!-- link the "text-sense" bark_1 to the word node -->
   <ReferenceLink>
      <WordNode name="#bark">
      <ConceptNode = "bark_1">
   </ReferenceLink>

------]]

Sentence identification
-----------------------
Multiple alternate parses of a sentence should be associated with
a single sentence. This may be accomplished by declaring a sentence:

   SENTENCE_NODE <- CONCEPT_NODE

   <SentenceNode name="sentence_22" strength=0.9 confidence=0.6/>

and associating it with each parse:

   <ParseLink>
      <ConceptNode name="parse_1" strength=0.9 confidence=0.6/>
      <SentenceNode name="sentence_22" />
   </ParseLink>

   <ParseLink>
      <ConceptNode name="parse_2" strength=0.8 confidence=0.5/>
      <SentenceNode name="sentence_22" />
   </ParseLink>

and so on. The special node type "SentenceNode" is used in order to
easily locate new sentences added to the system. After word processing
has been performed, the sentence node is marked to indicate that 
processing is complete, as so:

   <InheritanceLink>
      <SentenceNode name="sentence_22" />
      <ConceptNode name="wsd_completed" />
   </InheritanceLink>

Word concepts
-------------
This section elaborates on point 5 above.

The goal of word-sense disambiguation is to asociate a word instance to
each possible word-sense, and then to determine the most-likely pairing.
The Mihalcea algorithm[1] performs this by creating weighted links
between (word-instance, word-sense) pairs. (These are refered to as
"admissible labels" in [1]).  The goal of this section is
to describe the opencog representation of these pairs-of-pairs. 
Crudely speaking, these relations form an adjacency matrix for a graph
resembling Markov chain; and so the representation problem is that of 
representing a square matrix in OpenCog.  Several representiatons are
reviewed.

A naive way of representing word-concept links would be to create links:

   <InheritanceLink strength=0.9 confidence=0.1>
      <ConceptNode name="bark_144" />
      <WordSenseNode name="bark_sense_23" />
   </InheritanceLink>

The likelihood "bark_sense_23" being the correct sense for the word
"bark" is encoded in the strength of the Inheritance Link.

The pairs-of-pairs relations can then be provided as below. Here, the
example sentence contains the words "tree" and "bark", which can then
mutually reinforce the most likely meaning of each word.

   <!-- the word "tree" occured in the sentence -->
   <CosenseLink strength=0.49 confidence=0.3>
      <InheritanceLink strength=0.9 confidence=0.6>
         <ConceptNode name="tree_99" />
         <WordSenseNode name="tree_sense_12" />
      </InheritanceLink>
      
      <InheritanceLink strength=0.9 confidence=0.1>
         <ConceptNode name="bark_144" />
         <WordSenseNode name="bark_sense_23" />
      </InheritanceLink>
   </CosenseLink>

Note that the set link is unordered: there is no prefered direction in
the relation between "tree" and "bark".

[[An alternate representation for the above would create a single node
for every (word-instance, word-sense) pair, like so:

   <ConceptNode name="bark_pair_1"  strength=0.9 confidence=0.1>

The likelihood "bark_sense_23" being the correct sense for the word
"bark" is encoded in the strength of this node. This node is  
associated with the (word-instance, word-sense) pair as follows:

   <AssociativeLink>
      <ConceptNode name="bark_pair_1"  strength=0.9 confidence=0.1>
      <WordSenseNode name="bark_sense_23" />
   </AssociativeLink>

   <AssociativeLink>
      <ConceptNode name="bark_144" />
      <ConceptNode name="bark_pair_1"  strength=0.9 confidence=0.1>
   </AssociativeLink>

The associative links are not weighted. The pairs-of-pairs are then
represented as follows:

   <SentenceCooccurrenceLink strength=0.49 confidence=0.3>
      <ConceptNode name="tree_pair_2"  strength=0.9 confidence=0.6>
      <ConceptNode name="bark_pair_1"  strength=0.9 confidence=0.1>
   </SentenceCooccurrenceLink>

Its not clear which representation is "better".]]

The Mihalcea algorithm
----------------------
This section describes how the Mihalcea algorithm[1] is actually
implemented in opencog.

-- "Admissible labels" -- in the text above, and in the code, these are
   refered to as (word-instance, word-sense) pairs. These are added to
   the raw relex input by class MihalceaLabel. They are added to all
   words in all parses.

-- "Build a graph of label dependencies". In the text above, and in the
   code, these are refered to as "pairs of pairs". These are added to 
   the by the class MihalceaEdge.  There is an importqnt difference:
   the original Mihalcea algorithm directs that these be added 
   word-by-word in the sentence. Here, instead, these are added between
   words participating between relex relations. In particular, because
   relex will omit determiners ("a", "the") and other words from 
   relations, there won't be edges between these. Relex will also
   sometimes collapse prepositional phrases into colocations, as well
   as entities, this will also cause a subtle difference in the graph
   structure.

-- "Label dependencies". Currently, only the Leacock-Chodorow word-sense
   similarity measure is implemented. t only provides a measure that
   relates nouns to nouns, and verbs to verbs. It says nothing about 
   adjectives or adverbs. It is implemented in class SenseSimilarity.


Propagation of probabilities
----------------------------




Domains of discourse (aka word-sense frequency counts)
------------------------------------------------------
Some word senses are more common than others. For example, archaic word 
senses tend to be rarely encountered ... except when one is reading an 
archaic text. Similarly, scientific and technical texts tend to employ 
certain common word with certain very uncommon meanings. Cool, hip,
edgy writing will also use words in uncomon senses, or in unusual
constructions.

An anecdote to illustrate the idea:

  "When I was young and learning French, I found it easy to read and 
   understand "Le Monde", and was utterly perplexed by "Paris Match",
   even thought Paris Match sentences were considerably shorter, and 
   seemed to use a simpler vocabulary -- alas, for me, the Paris Match
   lingo was heavily overladen with French pop culture references and 
   current events, which I was completely unaware of.  I would sit 
   there, with Paris Match, and look up every word in the dictionary, 
   even though I already knew the word, because I was searching for 
   some meaning... which, of course was not to be found in the 
   dictionary. By contrast, Le Monde was not trying to be hip, edgy
   or entertaining, it was trying to be precise and explicit, and in
   this, it succeeds.  The moral of the story is that word-sense 
   frequency counts for Le Monde and Paris Match would be radically 
   different -- and so, when I read a "typical" French text, say, 
   Le Canard Enchainee (a satirical political paper), or Alexander
   Dumas (The Three Musketeers-- eminently readable, but clearly 18th
   century French), which should I use?"

Wordnet does have the notion of discourse domains.

XXX ToBeResolved: How to employ context-sensitive word-frequency counts
is unclear.


References
----------
[1] Radu Mihalcea, "Unsupervised Large-Vocabulary Word Sense
    Disambiguation with Graph-based Algorithms for Sequence Data 
    Labeling", 2005

