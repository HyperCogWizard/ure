
                      Word Sense Disambiguation
                      -------------------------
                           Working Notes 
                      Linas Vepstas April 2008


This document contains working notes for the mapping of WordNet ideas
onto OpenCog structures. It also describes how words in a parsed
sentence are associated with different word senses.


Word senses
-----------
Associated with every word is a list of possible senses.
Consider the word "bark", thus:

    <WordNode name="bark" />   -- a word.

A "word sense" is a collection of information about a single semantic
sense of a word. This bag includes information about the part-of-speech
(noun, verb, etc.), example usages, pointers to synonym sets (synsets),
hyponyms, hypernyms, etc. A specific "word sense" will be tagged with a
unique identifier, which is then used to reference that sense.

In the OpenCog type hierarchy, (type.script)
   WORD_SENSE_LINK <- ASSOCIATIVE_LINK

A WordSenseLink couples the tag that will stand for the word sense,
to the word itself:

   <WordSenseLink>
      <WordNode name="bark" />
      <ConceptNode name="bark_sense_23" />
   </WordSenseLink>

The tag value "bark_sense_23" has no particular meaning in itself,
it is just some unique string used to identify the sense.

Word senses can be crudely categorized according to part-of-speech.
Thus, for example:

   <PartOfSpeechLink>
      <ConceptNode name="bark_sense_23" />
      <PartOfSpeechNode name="noun" />
   </PartOfSpeechLink>

The above introduces a new node type "PartOfSpeechNode", and a new link
type: "PartOfSpeechLink".  Other possible properties might include gender,
entity tags, and so on. This potentially leads to an explosion of 
special-purpose nodes. For example, there are over a dozen properties
that RelEx uses to tag words.  To avoid this explosion, the following 
syntax will be used:

   <EvaluationLink>
     <PredicateNode name="PartOfSpeech"/>
     <ListLink>
       <ConceptNode name="bark_sense_23" />
       <ConceptNode name="noun" />
     </ListLink>
   </EvaluationLink>

[[Square brackets set off design discussions; below follows some
commentary about alternate ways in which the above might have been
acheived.

The above specifies a triple that is to be associated together, the
triple ("bark_sense_23", PartOfSpeech, "noun").  Another possible way
of denoting a triple might have been:

   <PropertyLink>
     <ConceptNode name="bark_sense_23" />
     <ListLink>
       <ConceptNode name="PartOfSpeech"/>
       <ConceptNode name="noun" />
     </ListLink>
   </PropertyLink>

but this would require the invention of a new link type, "PropertyLink",
which other parts of the system are unaware of. Alternately, 

   <MemberLink>
     <ListLink>
       <ConceptNode name="PartOfSpeech"/>
       <ConceptNode name="noun" />
     </ListLink>
     <ConceptNode name="bark_sense_23" />
   </MemberLink>

could be useful, but this would loose the benefit of the predicate form.
A predicate could still be created later, by making use of a link from
the predicate name to a SatisfyingSetLink. ]]

Each of these nodes and links are assigned a truth value of 1 and a
confidence of 1. Non-unit truth-values & confidence will be used when 
relating word senses to the use of actual words in sentences. Here, 
however, the goal is to state "this word does have this as one possible
meaning, no matter how rarely used this meaning might be."

[[Some word senses are more common than others. For example, archaic
word senses tend to be rarely encountered ... except when one is 
reading an archaic text. This is addressed in a section below.]]

In WordNet, word senses are commonly established by giving "sister
terms" (a WordNet concept, equivalent to "synonyms for word senses")
as well as hypernyms, hyponyms, part holonyms, etc. A sister term,
hypernym, hyponym, etc. are thus links. Since some sister terms are 
closer than others, these links will have non-unit truth values. 

Hypernyms and hyponyms will be mapped to inheritance links.
A hypernym for the wood-bark is that it is a covering of something:

   <InheritanceLink strength=0.8 confidence=0.9 />
      <WordSenseNode name="bark_sense_23" />
      <WordSenseNode name="covering_sense_42" />
   </InheritanceLink>

A hyponym for wood-bark is that cork is a kind of bark:

   <InheritanceLink strength=0.8 confidence=0.9 />
      <WordSenseNode name="cork_sense_98" />
      <WordSenseNode name="bark_sense_23" />
   </InheritanceLink>

Note the reversed order between the hypo- and hyper- representations.


WordNet uses the phrase "sister term" instead of "synonym" to relate 
similar word senses. WordNet reserves the use of "synonym" for relating
words, not word senses. Related word senses will be linked with
SimilarityLinks

   SIMILARITY_LINK <- UNORDERED_LINK

WordNet includes the idea of "part holonyms", that something is a part
of something else, and its converse, the meronym, that something is
composed of parts.  

   PART_OF_LINK <- INHERITANCE_LINK

[[Making the part-of link derive from inheritance link seems dangerous,
since WordNet does distinguish is-a and kind-of relations, whereas the
above mapping erases it. This may need to be fixed. ]]

To continue with the tree-bark example:

   <!-- bark is a part of a branch -->
   <PartOfLink strength=0.8 confidence=0.9>
      <WordSenseNode name="bark_sense_23" />
      <WordSenseNode name="branch_sense_2" />
   </PartOfLink>

   <!-- bark is composed of lignin -->
   <PartOfLink strength=0.8 confidence=0.9>
      <WordSenseNode name="lignin_sense_1" />
      <WordSenseNode name="bark_sense_23" />
   </PartOfLink>

Finally, WordNet has the concept of "instance-of" relations.  These
will use InstanceLinks:

   INSTANCE_LINK <- ORDERED_LINK


Sentences to meanings
---------------------
Consider two sentences: "The loud bark woke me up." or "The rough bark
cut my finger." The goal of word-sense disambiguation is two-fold:
associate to all possible meanings of the word "bark", and then, rank
the possible meanings from most likely to least.

There are six related ideas of a word, and its sense, that need to 
be distinguished. These are:

1) The "word instance", as it occurs in the sentence. Word instances
   may be mis-spelled, and thus it may be ambiguous as to which 
   actual dictionary word was intended.

2) The "dictionary word", the word as it would be in the standard
   lexicon of a language: a word in its proper spelling, and with
   morphological inflections stripped away: the root form of the word.

   Dictionary-words are represented by WordNode.

3) The "word sense", a particular sense of a word, as it might appear
   in a dictionary, or in WordNet. For example, "bark" in the sense of
   "tree bark". A word-sense exists independently of a particular
   sentence or text.

4) The "word parse", a linkage of a word-instance to a particular
   sentence parse. In the course of parsing, syntactical information
   is assigned to the word-parse: the word parse is identified by a
   a part-of-speech, a tense, a number, as a subject or object. The
   assignment of these properties may vary from parse to parse.
   For any one given sentence, there may be as many different
   word-parses as there are sentence parses. Typically, most sentence
   parses will assign the same word properties to each word-parse;
   these can then be collapsed into one, with the realization that the
   identification of the word-parse is unambiguous across all of the
   different parses.

   Note that a word-parse is also an assignment of a word-instance to
   a dictionary-word. Although very rare, there are words which the
   English language which can plausibly be one of two different roots, 
   and that different sentence parses might assign different roots.
   This can occur more frequently if a word was mis-spelled: for 
   instance, using "there" instead of "their". An incorrect parse
   would link the mis-spelled word-instance to the spelled-the-same
   but-wrong dictionary-word; the correct parse would link the 
   word-instance to its correct spelling.

5) The "word concept", the linkage of a word sense to a word parse.
   The "word concept" is distinguished from the "word sense", as being 
   a particular word sense within context of the sentence in which it 
   appears. Word-concepts may be uncertain or ambiguous: the meaning of
   any particular word-instance, as it appears in a sentence, may not be
   entirely clear. Initially, a link is created between a word-parse and
   every possible appropriate word-sense (based on the part-of-speech
   assignment, etc.). Each link is assigned equal weight. The goal of 
   word-sense disambiguation is to (strongly) prefer one particular
   word-concept over another.

6) Finally, there is a "text-sense", which is the disembodied concept
   that a word instance is referring to, across the entire text. For
   example, in the text "Mary looked at the glass. She picked it up."
   the word-instances "she" and "Mary" both refer to the same
   text-sense, even though they are represented by completely different
   surface words. The relation of text-senses to a text is also
   probabilistic: there is a non-zero probability that "she" does 
   not actually refer to Mary.  At the basic level, a text-sense 
   is a link between a node "Mary the abstract concept" and various 
   word-concepts. 

   Thus, for example, the word "Mary" has multiple possible word-senses:
   "Mary" may be a human being, but also, "Mary" may be a dog or other 
   animal, or "Mary" may be a statue of the the Virgin Mary.  The act of 
   word-sense disambiguation will assign a probability to the 
   word-concepts binding "Mary" to each of these possible word-senses.
   Then, the likelihood that "she" refers to "Mary" depends on the 
   likelihood that "Mary" refers to a statue of the Virgin Mary, as
   statues cannot pick things up. 

   (Incidentally, this gives some mathematical insight into humour,
   poetry, and "magical" and fantasy literature: in the normal world,
   a statue cannot pick things up. But a well-directed joke can force
   one to conclude that it must have: an impossibility, and therefore
   funny. A "well-directed" joke does this by making sure that there
   is no ambiguity in either the parse, or in the associated
   word-senses, so that there is no question, upon arriving at the punch
   line, of what it implies, however absurd that may be.)

To illustrate the above:

Consider the example text: "We stood in the yard.  As she walked up to
me, I laid my hand on the old oak.  Its rough bark cut my finger."
This sentence contains the word-instance "bark". The word-parse of
"bark" is unambiguous: it is a noun.  From WordNet, we know that 
"bark-the-noun" has several different word-senses: it may be tree-bark,
or it may be a type of watercraft.  The word-concept would be 
a linkage from the word-parse "bark-the-noun" to the sense of "tree-bark";
the goal of word-sense disambiguation is to give this particular 
word-concept a high probability and a high confidence, as compared to
"bark-the-watercraft".  Lastly, there is the text-sense. In this text, 
the text-concept for "bark" can be deduced to be "the bark of an oak
tree in a particular yard in which there are two people".

Network structure
-----------------
The goal of the OpenCog network structure given below is to properly 
capture the above-described relationships, inserting probabilities 
in all of the right places, and allowing an algorithm to walk the
network of connections and assign the highest probabilities to the most
likely interpretations.

The basic algorithmic components are as follows:
A) Import WordNet data to establish "dictionary-words" and "word-senses" 
   as defined above.
B) Use the Link-Grammar+RelEx infrastructure to identify 
   "word-instances", and link them to "word-parses", and to assign
   initial probabilities ranking alternative parses.
C) Use Radu Mihalcea's word-sense-disambiguation algorithms to compute
   initial probability assignments for "word-concepts".
D) Use anaphora resolution algorithms to assign initial probabilities
   to text-senses.
E) Use "common-sense reasoning" to refine probabilities for text-senses.
   A given text-sense probability can act as feedback for the
   likelihood of a given parse being correct, and so the initial
   probabilities assigned in steps B,C,D are then iterated upon,
   in an effort to find convergence.


Word-parse meaning
------------------
Word-parses assign syntactic meaning to a word, within the context of a
parse. Thus, for example, the goal is to express "in parse 2, the word
'bark' has part-of-speech 'noun'"

This suggests the structure below:
   <ConceptNode name="parse_1" strength=0.9 confidence=0.6/>
   <ConceptNode name="parse_2" strength=0.8 confidence=0.5/>
   <ConceptNode name="parse_3" strength=0.6 confidence=0.3/>

   <!-- in parse 2, the word "bark" is associated with bark_144 -->
   <InheritanceLink>
      <WordInstanceNode name="bark_word_instance_4" />
      <ConceptNode name="bark_144" />
   </InheritanceLink>

   <!-- in parse 3, the word "bark" is associated with bark_169 -->
   <InheritanceLink>
      <WordInstanceNode name="bark_word_instance_4" />
      <ConceptNode name="bark_169" />
   </InheritanceLink>

   <!-- Identify bark_144 as being parse instance 2 -->
   <EvaluationLink>
     <PredicateNode name="ParseInstance"/>
     <ListLink>
       <ConceptNode name="bark_144" />
       <ConceptNode name="parse_2" t/>
     </ListLink>
   </EvaluationLink>

   <!-- In parse 2, which has bark_144, it is a noun -->
   <EvaluationLink>
     <PredicateNode name="PartOfSpeech"/>
     <ListLink>
       <ConceptNode name="bark_144" />
       <ConceptNode name="noun" />
     </ListLink>
   </EvaluationLink>

   <!-- In parse 3, which has bark_169, it is a verb -->
   <EvaluationLink>
     <PredicateNode name="PartOfSpeech"/>
     <ListLink>
       <ConceptNode name="bark_169" />
       <ConceptNode name="noun" />
     </ListLink>
   </EvaluationLink>

------
The current RelEx output is as follows, but this seems somehow
not quite right, in light of the above discussions.

   <!-- link the word-instance to the "text-sense" bark_1 -->
   <InheritanceLink>
      <WordInstanceNode name="bark_1234" />
      <ConceptNode name="bark_1" />
   </InheritanceLink>

   <!-- link the "text-sense" bark_1 to the word node -->
   <ReferenceLink>
      <WordNode name="#bark">
      <ConceptNode = "bark_1">
   </ReferenceLink>

------

XXX The below is totally wrong and is still being drafted.

It seems that RelEx should generate the following:

   <!-- link a word-instance to a word -->
   <ReferenceLink strength=1 confidence=1 >
      <WordInstanceNode name="bark_1234" />
      <WordNode name="#bark">
   </ReferenceLink>

The word-instance is tagged with a universal unique ID of "1234" for 
record-keeping purposes; it captures the notion that this is a specific
word in a specific sentence.  The strength and confidence are ranked
very high, to indicate that there is little or no question that the 
word in the sentence was "bark". This may not be the case if a word
is mis-spelled in the original sentence, and the mis-spelled word has
several possible spelling-interpretations.

RelEx word properties would be given as:

   <InheritanceLink strength=0.9 confidence=0.8>
      <WordInstanceNode name="bark_1234" />
      <DefinedLinguisticConceptNode name="#pos#noun" />
   </InheritanceLink>

These are given a lower truth value and confidence, to indicate that the
interpretation of a part-of-speech may depend on how the sentence was
parsed; the strength and confidence values are keyed of of the
parse-ranking of a sentence. XXX how to show parse-dep ??

At a later stage, the word-sense disambiguation code would generate
the extra nodes needed to encode the text concept, etc.  Thus:

   <InheritanceLink>
      <WordInstanceNode name="bark_1234" />
      <ConceptNode = "bark_sense_23">
   </InheritanceLink>


anaphora.



ContextLink ... should each parse have its own context link ??

Word-sense frequency counts
---------------------------
Some word senses are more common than others. For example, archaic word 
senses tend to be rarely encountered ... except when one is reading an 
archaic text. Similarly, scientific and technical texts tend to employ 
certain common word with certain very uncommon meanings. Cool, hip,
edgy writing will also use words in uncomon senses, or in unusual
constructions.

XXX ToBeResolved: How to employ context-sensitive word-frequency counts
is unclear.


